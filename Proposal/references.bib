
@article{bidelman_auditory-nerve_2011,
	title = {Auditory-nerve responses predict pitch attributes related to musical consonance-dissonance for normal and impaired hearing},
	volume = {130},
	issn = {1520-8524},
	doi = {10.1121/1.3605559},
	abstract = {Human listeners prefer consonant over dissonant musical intervals and the perceived contrast between these classes is reduced with cochlear hearing loss. Population-level activity of normal and impaired model auditory-nerve (AN) fibers was examined to determine (1) if peripheral auditory neurons exhibit correlates of consonance and dissonance and (2) if the reduced perceptual difference between these qualities observed for hearing-impaired listeners can be explained by impaired AN responses. In addition, acoustical correlates of consonance-dissonance were also explored including periodicity and roughness. Among the chromatic pitch combinations of music, consonant intervals/chords yielded more robust neural pitch-salience magnitudes (determined by harmonicity/periodicity) than dissonant intervals/chords. In addition, AN pitch-salience magnitudes correctly predicted the ordering of hierarchical pitch and chordal sonorities described by Western music theory. Cochlear hearing impairment compressed pitch salience estimates between consonant and dissonant pitch relationships. The reduction in contrast of neural responses following cochlear hearing loss may explain the inability of hearing-impaired listeners to distinguish musical qualia as clearly as normal-hearing individuals. Of the neural and acoustic correlates explored, AN pitch salience was the best predictor of behavioral data. Results ultimately show that basic pitch relationships governing music are already present in initial stages of neural processing at the AN level.},
	language = {eng},
	number = {3},
	journal = {J. Acoust. Soc. Am.},
	author = {Bidelman, Gavin M. and Heinz, Michael G.},
	month = sep,
	year = {2011},
	pmid = {21895089},
	pmcid = {PMC3188968},
	keywords = {Acoustic Stimulation, Animals, Audiometry, Pure-Tone, Auditory Threshold, Cats, Cochlea, Cochlear Nerve, Computer Simulation, Discrimination, Psychological, Evoked Potentials, Hearing Disorders, Humans, Models, Neurological, Music, Persons With Hearing Impairments, Pitch Perception, Time Factors},
	pages = {1488--1502},
	file = {Full Text:/home/sivaprakasaman/Zotero/storage/LIMPREH7/Bidelman and Heinz - 2011 - Auditory-nerve responses predict pitch attributes .pdf:application/pdf}
}

@article{peterson_contribution_2015,
	title = {Contribution of hearing aids to music perception by cochlear implant users},
	volume = {16 Suppl 3},
	issn = {1754-7628},
	doi = {10.1179/1467010015Z.000000000268},
	abstract = {OBJECTIVES: Modern cochlear implant (CI) encoding strategies represent the temporal envelope of sounds well but provide limited spectral information. This deficit in spectral information has been implicated as a contributing factor to difficulty with speech perception in noisy conditions, discriminating between talkers and melody recognition. One way to supplement spectral information for CI users is by fitting a hearing aid (HA) to the non-implanted ear.
METHODS: In this study 14 postlingually deaf adults (half with a unilateral CI and the other half with a CI and an HA (CI + HA)) were tested on measures of music perception and familiar melody recognition.
RESULTS: CI + HA listeners performed significantly better than CI-only listeners on all pitch-based music perception tasks. The CI + HA group did not perform significantly better than the CI-only group in the two tasks that relied on duration cues. Recognition of familiar melodies was significantly enhanced for the group wearing an HA in addition to their CI. This advantage in melody recognition was increased when melodic sequences were presented with the addition of harmony.
CONCLUSION: These results show that, for CI recipients with aidable hearing in the non-implanted ear, using a HA in addition to their implant improves perception of musical pitch and recognition of real-world melodies.},
	language = {eng},
	journal = {Cochlear Implants Int},
	author = {Peterson, Nathaniel and Bergeson, Tonya R.},
	month = sep,
	year = {2015},
	pmid = {26561890},
	keywords = {Noise, Music perception, Cochlear implant, Deafness, Hearing aid, Female, Male, Humans, Hearing Aids, Auditory Perception, Music, Aged, Middle Aged, Cochlear Implantation, Cochlear Implants, Electroacoustic hearing, Combined Modality Therapy, Correction of Hearing Impairment},
	pages = {S71--78}
}

@article{xu_relative_2003,
	title = {Relative importance of temporal envelope and fine structure in lexical-tone perception ({L})},
	volume = {114},
	issn = {0001-4966},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1283139/},
	abstract = {The relative importance of temporal envelope and fine structure in speech and music perception was investigated by Smith et al. [Nature (London) 416, 87–90 (2002)] using "auditory chimera" in which the envelope from one sound was paired with the fine structure of another. Smith et al. found that, when 4 to 16 frequency bands were used, recognition of English speech was dominated by the envelope, whereas recognition of melody was dominated by the fine structure. In the present study, Mandarin Chinese monosyllables were divided into 4, 8, or 16 frequency bands and the fine structure and envelope of one tone pattern were exchanged with those of another tone pattern of the same monosyllable. Five normal-hearing native Mandarin Chinese speakers completed a four-alternative forced-choice tone-identification task. In the vast majority of trials, subjects based their identification of the monosyllables on the fine structure rather than the envelope. Thus, the relative importance of envelope and fine structure for lexical-tone perception resembled that for melody recognition rather than that for English speech recognition. Delivering fine-structure information in cochlear implant stimulation could be particularly beneficial for lexical-tone perception.},
	number = {6 Pt 1},
	urldate = {2020-10-30},
	journal = {J Acoust Soc Am},
	author = {Xu, Li and Pfingst, Bryan E.},
	month = dec,
	year = {2003},
	pmid = {14714781},
	pmcid = {PMC1283139},
	pages = {3024--3027},
	file = {PubMed Central Full Text PDF:/home/sivaprakasaman/Zotero/storage/L9WC3FRI/Xu and Pfingst - 2003 - Relative importance of temporal envelope and fine .pdf:application/pdf}
}

@article{arnoldner_speech_2007,
	title = {Speech and music perception with the new fine structure speech coding strategy: preliminary results},
	volume = {127},
	issn = {0001-6489},
	shorttitle = {Speech and music perception with the new fine structure speech coding strategy},
	doi = {10.1080/00016480701275261},
	abstract = {CONCLUSIONS: Taking into account the excellent results with significant improvements in the speech tests and the very high satisfaction of the patients using the new strategy, this first implementation of a fine structure strategy could offer a new quality of hearing with cochlear implants (CIs).
OBJECTIVE: This study consisted of an intra-individual comparison of speech recognition, music perception and patient preference when subjects used two different speech coding strategies with a MedEl Pulsar CI: continuous interleaved sampling (CIS) and the new fine structure processing (FSP) strategy. In contrast to envelope-based strategies, the FSP strategy also delivers subtle pitch and timing differences of sound to the user and is thereby supposed to enhance speech perception in noise and increase the quality of music perception.
PATIENTS AND METHODS: This was a prospective study assessing performance with two different speech coding strategies. The setting was a CI programme at an academic tertiary referral centre. Fourteen post-lingually deaf patients using a MedEl Pulsar CI with a mean CI experience of 0.98 years were supplied with the new FSP speech coding strategy. Subjects consecutively used the two different speech coding strategies. Speech and music tests were performed with the previously fitted CIS strategy, immediately after fitting with the new FSP strategy and 4, 8 and 12 weeks later. The main outcome measures were individual performance and subjective assessment of two different speech processors.
RESULTS: Speech and music test scores improved statistically significantly after conversion from CIS to FSP strategy. Twelve of 14 patients preferred the new FSP speech processing strategy over the CIS strategy.},
	language = {eng},
	number = {12},
	journal = {Acta Otolaryngol},
	author = {Arnoldner, Christoph and Riss, Dominik and Brunner, Markus and Durisin, Martin and Baumgartner, Wolf-Dieter and Hamzavi, Jafar-Sasan},
	month = dec,
	year = {2007},
	pmid = {17851892},
	keywords = {Female, Male, Humans, Adult, Speech Perception, Music, Aged, Middle Aged, Cochlear Implants},
	pages = {1298--1303}
}

@article{riquimaroux_perception_2006,
	title = {Perception of noise-vocoded speech sounds: {Sentences}, words, accents and melodies},
	volume = {27},
	shorttitle = {Perception of noise-vocoded speech sounds},
	doi = {10.1250/ast.27.325},
	abstract = {Recent works on perception of noise-vocoded speech sound (NVSS) have revealed that amplitude envelope information is very important for speech perception when spectral information is not sufficiently available. Basically, the fundamental frequency information is not available and formant peaks cannot not be identified in NVSS. However, we can even recognize accent and distinguish male voice from female voice in NVSS. More, melody can be created from lyrics once lyrics are intelligible. In the present study, findings from fMRI measurement are introduced to show neural activities in the central nervous system during listening to NVSS. The present data indicate that various sites in the brain, which are not ordinarily used for speech recognition, participate in making NVSS intelligible. Applications of the present work include an innovative speech processor and a training system for hearing impaired people.},
	number = {6},
	journal = {Acoustical Science and Technology},
	author = {Riquimaroux, Hiroshi},
	year = {2006},
	keywords = {Speech perception, Amplitude envelope, Brain plasticity, Functional MRI, Noise-vocoded speech sounds},
	pages = {325--331},
	file = {J-Stage - Snapshot:/home/sivaprakasaman/Zotero/storage/9C2TNU2A/en.html:text/html;Full Text PDF:/home/sivaprakasaman/Zotero/storage/TNTENAZT/Riquimaroux - 2006 - Perception of noise-vocoded speech sounds Sentenc.pdf:application/pdf}
}

@article{heng_impaired_2011,
	title = {Impaired perception of temporal fine structure and musical timbre in cochlear implant users},
	volume = {280},
	issn = {0378-5955},
	url = {http://www.sciencedirect.com/science/article/pii/S0378595511001560},
	doi = {10.1016/j.heares.2011.05.017},
	abstract = {Cochlear implant (CI) users demonstrate severe limitations in perceiving musical timbre, a psychoacoustic feature of sound responsible for ‘tone color’ and one’s ability to identify a musical instrument. The reasons for this limitation remain poorly understood. In this study, we sought to examine the relative contributions of temporal envelope and fine structure for timbre judgments, in light of the fact that speech processing strategies employed by CI systems typically employ envelope extraction algorithms. We synthesized “instrumental chimeras” that systematically combined variable amounts of envelope and fine structure in 25\% increments from two different source instruments with either sustained or percussive envelopes. CI users and normal hearing (NH) subjects were presented with 150 chimeras and asked to determine which instrument the chimera more closely resembled in a single-interval two-alternative forced choice task. By combining instruments with similar and dissimilar envelopes, we controlled the valence of envelope for timbre identification and compensated for envelope reconstruction from fine structure information. Our results show that NH subjects utilize envelope and fine structure interchangeably, whereas CI subjects demonstrate overwhelming reliance on temporal envelope. When chimeras were created from dissimilar envelope instrument pairs, NH subjects utilized a combination of envelope (p = 0.008) and fine structure information (p = 0.009) to make timbre judgments. In contrast, CI users utilized envelope information almost exclusively to make timbre judgments (p {\textless} 0.001) and ignored fine structure information (p = 0.908). Interestingly, when the value of envelope as a cue was reduced, both NH subjects and CI users utilized fine structure information to make timbre judgments (p {\textless} 0.001), although the effect was quite weak in CI users. Our findings confirm that impairments in fine structure processing underlie poor perception of musical timbre in CI users.},
	language = {en},
	number = {1},
	urldate = {2020-11-01},
	journal = {Hearing Research},
	author = {Heng, Joseph and Cantarero, Gabriela and Elhilali, Mounya and Limb, Charles J.},
	month = oct,
	year = {2011},
	pages = {192--200},
	file = {ScienceDirect Full Text PDF:/home/sivaprakasaman/Zotero/storage/DUPUCZDZ/Heng et al. - 2011 - Impaired perception of temporal fine structure and.pdf:application/pdf;ScienceDirect Snapshot:/home/sivaprakasaman/Zotero/storage/9PHRDBM9/S0378595511001560.html:text/html}
}

@article{manno_uncertain_2019,
	title = {Uncertain {Emotion} {Discrimination} {Differences} {Between} {Musicians} and {Non}-musicians {Is} {Determined} by {Fine} {Structure} {Association}: {Hilbert} {Transform} {Psychophysics}},
	volume = {13},
	issn = {1662-4548},
	shorttitle = {Uncertain {Emotion} {Discrimination} {Differences} {Between} {Musicians} and {Non}-musicians {Is} {Determined} by {Fine} {Structure} {Association}},
	doi = {10.3389/fnins.2019.00902},
	abstract = {Humans perceive musical sound as a complex phenomenon, which is known to induce an emotional response. The cues used to perceive emotion in music have not been unequivocally elucidated. Here, we sought to identify the attributes of sound that confer an emotion to music and determine if professional musicians have different musical emotion perception than non-musicians. The objective was to determine which sound cues are used to resolve emotional signals. Happy or sad classical music excerpts modified in fine structure or envelope conveying different degrees of emotional certainty were presented. Certainty was determined by identification of the emotional characteristic presented during a forced-choice discrimination task. Participants were categorized as good or poor performers (n = 32, age 21.16 ± 2.59 SD) and in a separate group as musicians in the first or last year of music education at a conservatory (n = 32, age 21.97 ± 2.42). We found that temporal fine structure information is essential for correct emotional identification. Non-musicians used less fine structure information to discriminate emotion in music compared with musicians. The present psychophysical experiments revealed what cues are used to resolve emotional signals and how they differ between non-musicians and musically educated individuals.},
	language = {eng},
	journal = {Front Neurosci},
	author = {Manno, Francis A. M. and Cruces, Raul R. and Lau, Condon and Barrios, Fernando A.},
	year = {2019},
	pmid = {31619943},
	pmcid = {PMC6759500},
	keywords = {envelope, psychophysics, fine structure, amplitude, emotion, frequency, modulation},
	pages = {902},
	file = {Full Text:/home/sivaprakasaman/Zotero/storage/IYT2HD8E/Manno et al. - 2019 - Uncertain Emotion Discrimination Differences Betwe.pdf:application/pdf}
}

@article{parida_spectrally_2020,
	title = {Spectrally specific temporal analyses of spike-train responses to complex sounds: {A} unifying framework},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Spectrally specific temporal analyses of spike-train responses to complex sounds},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.17.208330v1},
	doi = {10.1101/2020.07.17.208330},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Significant scientific and translational questions remain in auditory neuroscience surrounding the neural correlates of perception. Relating perceptual and neural data collected from humans can be useful; however, human-based neural data are typically limited to evoked far-field responses, which lack anatomical and physiological specificity. Laboratory-controlled preclinical animal models offer the advantage of comparing single-unit and evoked responses from the same animals. This ability provides opportunities to develop invaluable insight into proper interpretations of evoked responses, which benefits both basic-science studies of neural mechanisms and translational applications, e.g., diagnostic development. However, these comparisons have been limited by a disconnect between the types of spectrotemporal analyses used with single-unit spike trains and evoked responses, which results because these response types are fundamentally different (point-process versus continuous-valued signals) even though the responses themselves are related. Here, we describe a unifying framework to study temporal coding of complex sounds that allows spike-train and evoked-response data to be analyzed and compared using the same advanced signal-processing techniques. The framework uses alternating-polarity peristimulus-time histograms computed from single-unit spike trains to allow advanced spectral analyses of both slow (envelope) and rapid (temporal fine structure) response components. Demonstrated benefits include: (1) generalization beyond classic metrics of temporal coding, e.g., vector strength and correlograms, (2) novel spectrally specific temporal-coding measures that are less corrupted by distortions due to hair-cell transduction, synaptic rectification, and neural stochasticity compared to previous metrics, e.g., the correlogram peak-height, (3) spectrally specific analyses of spike-train modulation coding that can be directly compared to perceptually based models of speech intelligibility, and (4) superior spectral resolution in analyzing the neural representation of nonstationary sounds, such as speech and music. This unifying framework significantly expands the potential of preclinical animal models to advance our understanding of the physiological correlates of perceptual deficits in real-world listening following sensorineural hearing loss.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Author summary{\textless}/h3{\textgreater} {\textless}p{\textgreater}Despite major technological and computational advances, we remain unable to match human auditory perception using machines, or to restore normal-hearing communication for those with sensorineural hearing loss. An overarching reason for these limitations is that the neural correlates of auditory perception, particularly for complex everyday sounds, remain largely unknown. Although neural responses can be measured in humans noninvasively and compared with perception, these evoked responses lack the anatomical and physiological specificity required to reveal underlying neural mechanisms. Single-unit spike-train responses can be measured from preclinical animal models with well-specified pathology; however, the disparate response types (point-process versus continuous-valued signals) have limited application of the same advanced signal-processing analyses to single-unit and evoked responses required for direct comparison. Here, we fill this gap with a unifying framework for analyzing both spike-train and evoked neural responses using advanced spectral analyses of both the slow and rapid response components that are known to be perceptually relevant for speech and music, particularly in challenging listening environments. Numerous benefits of this framework are demonstrated here, which support its potential to advance the translation of spike-train data from animal models to improve clinical diagnostics and technological development for real-world listening.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-11-01},
	journal = {bioRxiv},
	author = {Parida, Satyabrata and Bharadwaj, Hari and Heinz, Michael G.},
	month = jul,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.07.17.208330},
	file = {Full Text PDF:/home/sivaprakasaman/Zotero/storage/YQ2KS3LY/Parida et al. - 2020 - Spectrally specific temporal analyses of spike-tra.pdf:application/pdf;Snapshot:/home/sivaprakasaman/Zotero/storage/LEQG7T4K/2020.07.17.html:text/html}
}

@article{bruce_phenomenological_2018,
	series = {Computational models of the auditory system},
	title = {A phenomenological model of the synapse between the inner hair cell and auditory nerve: {Implications} of limited neurotransmitter release sites},
	volume = {360},
	issn = {0378-5955},
	shorttitle = {A phenomenological model of the synapse between the inner hair cell and auditory nerve},
	url = {http://www.sciencedirect.com/science/article/pii/S0378595517303696},
	doi = {10.1016/j.heares.2017.12.016},
	abstract = {Peterson and Heil [Hear. Res., In Press] have argued that the statistics of spontaneous spiking in auditory nerve fibers (ANFs) can be best explained by a model with a limited number of synaptic vesicle docking (release) sites (∼4) and a relatively-long average redocking time (∼16–17 ms) for each of the sites. In this paper we demonstrate how their model can be: i) generalized to also describe sound-driven ANF responses and ii) incorporated into a well-established and widely-used model of the entire auditory periphery [Zilany et al., J. Acoust. Soc. Am. 135, 283–286, 2014]. The responses of the new model exhibit substantial improvement in several measures of ANF spiking statistics, and predicted physiological forward-masking and rate-level functions from the new model structure are shown to also better match published physiological data.},
	language = {en},
	urldate = {2020-11-01},
	journal = {Hearing Research},
	author = {Bruce, Ian C. and Erfani, Yousof and Zilany, Muhammad S. A.},
	month = mar,
	year = {2018},
	keywords = {Synapse, Auditory model, Neurotransmitter vesicle release, Refractoriness, Renewal process, Spike timing statistics},
	pages = {40--54},
	file = {ScienceDirect Full Text PDF:/home/sivaprakasaman/Zotero/storage/QH4EDU5C/Bruce et al. - 2018 - A phenomenological model of the synapse between th.pdf:application/pdf;ScienceDirect Snapshot:/home/sivaprakasaman/Zotero/storage/YFK2NZKS/S0378595517303696.html:text/html}
}

@misc{noauthor_sound_nodate,
	title = {Sound samples: https://philharmonia.co.uk/resources/sound-samples/},
	url = {https://philharmonia.co.uk/resources/sound-samples/},
	language = {en-GB},
	urldate = {2020-11-01},
	journal = {Philharmonia},
	file = {Snapshot:/home/sivaprakasaman/Zotero/storage/IU48YD57/sound-samples.html:text/html}
}

@article{li_improved_2012,
	title = {Improved perception of speech in noise and {Mandarin} tones with acoustic simulations of harmonic coding for cochlear implants},
	volume = {132},
	issn = {0001-4966},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3505211/},
	doi = {10.1121/1.4756827},
	abstract = {Harmonic and temporal fine structure (TFS) information are important cues for speech perception in noise and music perception. However, due to the inherently coarse spectral and temporal resolution in electric hearing, the question of how to deliver harmonic and TFS information to cochlear implant (CI) users remains unresolved. A harmonic-single-sideband-encoder [(HSSE); Nie et al. (2008). Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing; Lie et al., (2010). Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing] strategy has been proposed that explicitly tracks the harmonics in speech and transforms them into modulators conveying both amplitude modulation and fundamental frequency information. For unvoiced speech, HSSE transforms the TFS into a slowly varying yet still noise-like signal. To investigate its potential, four- and eight-channel vocoder simulations of HSSE and the continuous-interleaved-sampling (CIS) strategy were implemented, respectively. Using these vocoders, five normal-hearing subjects’ speech recognition performance was evaluated under different masking conditions; another five normal-hearing subjects’ Mandarin tone identification performance was also evaluated. Additionally, the neural discharge patterns evoked by HSSE- and CIS-encoded Mandarin tone stimuli were simulated using an auditory nerve model. All subjects scored significantly higher with HSSE than with CIS vocoders. The modeling analysis demonstrated that HSSE can convey temporal pitch cues better than CIS. Overall, the results suggest that HSSE is a promising strategy to enhance speech perception with CIs.},
	number = {5},
	urldate = {2020-11-01},
	journal = {J Acoust Soc Am},
	author = {Li, Xing and Nie, Kaibao and Imennov, Nikita S. and Won, Jong Ho and Drennan, Ward R. and Rubinstein, Jay T. and Atlas, Les E.},
	month = nov,
	year = {2012},
	pmid = {23145619},
	pmcid = {PMC3505211},
	pages = {3387--3398},
	file = {PubMed Central Full Text PDF:/home/sivaprakasaman/Zotero/storage/N7ZZQFPL/Li et al. - 2012 - Improved perception of speech in noise and Mandari.pdf:application/pdf}
}

@article{lee_timbre_2020,
	title = {The {Timbre} {Perception} {Test} ({TPT}): {A} new interactive musical assessment tool to measure timbre perception ability},
	volume = {82},
	issn = {1943-3921},
	shorttitle = {The {Timbre} {Perception} {Test} ({TPT})},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7536169/},
	doi = {10.3758/s13414-020-02058-3},
	abstract = {To date, tests that measure individual differences in the ability to perceive musical timbre are scarce in the published literature. The lack of such tool limits research on how timbre, a primary attribute of sound, is perceived and processed among individuals. The current paper describes the development of the Timbre Perception Test (TPT), in which participants use a slider to reproduce heard auditory stimuli that vary along three important dimensions of timbre: envelope, spectral flux, and spectral centroid. With a sample of 95 participants, the TPT was calibrated and validated against measures of related abilities and examined for its reliability. The results indicate that a short-version (8 minutes) of the TPT has good explanatory support from a factor analysis model, acceptable internal reliability (α = .69, ωt = .70), good test–retest reliability (r = .79) and substantial correlations with self-reported general musical sophistication (ρ = .63) and pitch discrimination (ρ = .56), as well as somewhat lower correlations with duration discrimination (ρ = .27), and musical instrument discrimination abilities (ρ = .33). Overall, the TPT represents a robust tool to measure an individual’s timbre perception ability. Furthermore, the use of sliders to perform a reproductive task has shown to be an effective approach in threshold testing. The current version of the TPT is openly available for research purposes.},
	number = {7},
	urldate = {2020-11-01},
	journal = {Atten Percept Psychophys},
	author = {Lee, Harin and Müllensiefen, Daniel},
	year = {2020},
	pmid = {32529570},
	pmcid = {PMC7536169},
	pages = {3658--3675},
	file = {PubMed Central Full Text PDF:/home/sivaprakasaman/Zotero/storage/YPQKVMPE/Lee and Müllensiefen - 2020 - The Timbre Perception Test (TPT) A new interactiv.pdf:application/pdf}
}

@article{heinz_quantifying_2009,
	title = {Quantifying envelope and fine-structure coding in auditory nerve responses to chimaeric speech},
	volume = {10},
	issn = {1438-7573},
	doi = {10.1007/s10162-009-0169-8},
	abstract = {Any sound can be separated mathematically into a slowly varying envelope and rapidly varying fine-structure component. This property has motivated numerous perceptual studies to understand the relative importance of each component for speech and music perception. Specialized acoustic stimuli, such as auditory chimaeras with the envelope of one sound and fine structure of another have been used to separate the perceptual roles for envelope and fine structure. Cochlear narrowband filtering limits the ability to isolate fine structure from envelope; however, envelope recovery from fine structure has been difficult to evaluate physiologically. To evaluate envelope recovery at the output of the cochlea, neural cross-correlation coefficients were developed that quantify the similarity between two sets of spike-train responses. Shuffled auto- and cross-correlogram analyses were used to compute separate correlations for responses to envelope and fine structure based on both model and recorded spike trains from auditory nerve fibers. Previous correlogram analyses were extended to isolate envelope coding more effectively in auditory nerve fibers with low center frequencies, which are particularly important for speech coding. Recovered speech envelopes were present in both model and recorded responses to one- and 16-band speech fine-structure chimaeras and were significantly greater for the one-band case, consistent with perceptual studies. Model predictions suggest that cochlear recovered envelopes are reduced following sensorineural hearing loss due to broadened tuning associated with outer-hair cell dysfunction. In addition to the within-fiber cross-stimulus cases considered here, these neural cross-correlation coefficients can also be used to evaluate spatiotemporal coding by applying them to cross-fiber within-stimulus conditions. Thus, these neural metrics can be used to quantitatively evaluate a wide range of perceptually significant temporal coding issues relevant to normal and impaired hearing.},
	language = {eng},
	number = {3},
	journal = {J Assoc Res Otolaryngol},
	author = {Heinz, Michael G. and Swaminathan, Jayaganesh},
	month = sep,
	year = {2009},
	pmid = {19365691},
	pmcid = {PMC3084379},
	keywords = {Acoustic Stimulation, Action Potentials, Animals, Chinchilla, Cochlea, Cochlear Implants, Cochlear Nerve, Computer Simulation, Evoked Potentials, Auditory, Models, Animal, Nerve Fibers, Speech},
	pages = {407--423},
	file = {Full Text:/home/sivaprakasaman/Zotero/storage/QSWLFINZ/Heinz and Swaminathan - 2009 - Quantifying envelope and fine-structure coding in .pdf:application/pdf}
}

@article{smith_chimaeric_2002,
	title = {Chimaeric sounds reveal dichotomies in auditory perception},
	volume = {416},
	issn = {0028-0836},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2268248/},
	doi = {10.1038/416087a},
	abstract = {By Fourier's theorem, signals can be decomposed into a sum of sinusoids of different frequencies. This is especially relevant for hearing, because the inner ear performs a form of mechanical Fourier transform by mapping frequencies along the length of the cochlear partition. An alternative signal decomposition, originated by Hilbert, is to factor a signal into the product of a slowly varying envelope and a rapidly varying fine time structure. Neurons in the auditory brainstem– sensitive to these features have been found in mammalian physiological studies. To investigate the relative perceptual importance of envelope and fine structure, we synthesized stimuli that we call ‘auditory chimaeras’, which have the envelope of one sound and the fine structure of another. Here we show that the envelope is most important for speech reception, and the fine structure is most important for pitch perception and sound localization. When the two features are in conflict, the sound of speech is heard at a location determined by the fine structure, but the words are identified according to the envelope. This finding reveals a possible acoustic basis for the hypothesized ‘what’ and ‘where’ pathways in the auditory cortex–.},
	number = {6876},
	urldate = {2020-11-01},
	journal = {Nature},
	author = {Smith, Zachary M. and Delgutte, Bertrand and Oxenham, Andrew J.},
	month = mar,
	year = {2002},
	pmid = {11882898},
	pmcid = {PMC2268248},
	pages = {87--90},
	file = {PubMed Central Full Text PDF:/home/sivaprakasaman/Zotero/storage/4DIPIAQG/Smith et al. - 2002 - Chimaeric sounds reveal dichotomies in auditory pe.pdf:application/pdf}
}
