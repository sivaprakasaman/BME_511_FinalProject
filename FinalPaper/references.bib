
@article{bidelman_auditory-nerve_2011,
	title = {Auditory-nerve responses predict pitch attributes related to musical consonance-dissonance for normal and impaired hearing},
	volume = {130},
	issn = {1520-8524},
	doi = {10.1121/1.3605559},
	abstract = {Human listeners prefer consonant over dissonant musical intervals and the perceived contrast between these classes is reduced with cochlear hearing loss. Population-level activity of normal and impaired model auditory-nerve (AN) fibers was examined to determine (1) if peripheral auditory neurons exhibit correlates of consonance and dissonance and (2) if the reduced perceptual difference between these qualities observed for hearing-impaired listeners can be explained by impaired AN responses. In addition, acoustical correlates of consonance-dissonance were also explored including periodicity and roughness. Among the chromatic pitch combinations of music, consonant intervals/chords yielded more robust neural pitch-salience magnitudes (determined by harmonicity/periodicity) than dissonant intervals/chords. In addition, AN pitch-salience magnitudes correctly predicted the ordering of hierarchical pitch and chordal sonorities described by Western music theory. Cochlear hearing impairment compressed pitch salience estimates between consonant and dissonant pitch relationships. The reduction in contrast of neural responses following cochlear hearing loss may explain the inability of hearing-impaired listeners to distinguish musical qualia as clearly as normal-hearing individuals. Of the neural and acoustic correlates explored, AN pitch salience was the best predictor of behavioral data. Results ultimately show that basic pitch relationships governing music are already present in initial stages of neural processing at the AN level.},
	language = {eng},
	number = {3},
	journal = {J. Acoust. Soc. Am.},
	author = {Bidelman, Gavin M. and Heinz, Michael G.},
	month = sep,
	year = {2011},
	pmid = {21895089},
	pmcid = {PMC3188968},
	keywords = {Acoustic Stimulation, Animals, Cats, Cochlea, Models, Neurological, Time Factors, Humans, Audiometry, Pure-Tone, Auditory Threshold, Hearing Disorders, Pitch Perception, Cochlear Nerve, Evoked Potentials, Computer Simulation, Discrimination, Psychological, Music, Persons With Hearing Impairments},
	pages = {1488--1502},
	file = {Full Text:/home/sivaprakasaman/Zotero/storage/LIMPREH7/Bidelman and Heinz - 2011 - Auditory-nerve responses predict pitch attributes .pdf:application/pdf}
}

@article{peterson_contribution_2015,
	title = {Contribution of hearing aids to music perception by cochlear implant users},
	volume = {16 Suppl 3},
	issn = {1754-7628},
	doi = {10.1179/1467010015Z.000000000268},
	abstract = {OBJECTIVES: Modern cochlear implant (CI) encoding strategies represent the temporal envelope of sounds well but provide limited spectral information. This deficit in spectral information has been implicated as a contributing factor to difficulty with speech perception in noisy conditions, discriminating between talkers and melody recognition. One way to supplement spectral information for CI users is by fitting a hearing aid (HA) to the non-implanted ear.
METHODS: In this study 14 postlingually deaf adults (half with a unilateral CI and the other half with a CI and an HA (CI + HA)) were tested on measures of music perception and familiar melody recognition.
RESULTS: CI + HA listeners performed significantly better than CI-only listeners on all pitch-based music perception tasks. The CI + HA group did not perform significantly better than the CI-only group in the two tasks that relied on duration cues. Recognition of familiar melodies was significantly enhanced for the group wearing an HA in addition to their CI. This advantage in melody recognition was increased when melodic sequences were presented with the addition of harmony.
CONCLUSION: These results show that, for CI recipients with aidable hearing in the non-implanted ear, using a HA in addition to their implant improves perception of musical pitch and recognition of real-world melodies.},
	language = {eng},
	journal = {Cochlear Implants Int},
	author = {Peterson, Nathaniel and Bergeson, Tonya R.},
	month = sep,
	year = {2015},
	pmid = {26561890},
	keywords = {Noise, Music perception, Cochlear implant, Deafness, Hearing aid, Female, Male, Humans, Hearing Aids, Auditory Perception, Music, Aged, Middle Aged, Cochlear Implantation, Cochlear Implants, Electroacoustic hearing, Combined Modality Therapy, Correction of Hearing Impairment},
	pages = {S71--78}
}

@article{xu_relative_2003,
	title = {Relative importance of temporal envelope and fine structure in lexical-tone perception ({L})},
	volume = {114},
	issn = {0001-4966},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC1283139/},
	abstract = {The relative importance of temporal envelope and fine structure in speech and music perception was investigated by Smith et al. [Nature (London) 416, 87–90 (2002)] using "auditory chimera" in which the envelope from one sound was paired with the fine structure of another. Smith et al. found that, when 4 to 16 frequency bands were used, recognition of English speech was dominated by the envelope, whereas recognition of melody was dominated by the fine structure. In the present study, Mandarin Chinese monosyllables were divided into 4, 8, or 16 frequency bands and the fine structure and envelope of one tone pattern were exchanged with those of another tone pattern of the same monosyllable. Five normal-hearing native Mandarin Chinese speakers completed a four-alternative forced-choice tone-identification task. In the vast majority of trials, subjects based their identification of the monosyllables on the fine structure rather than the envelope. Thus, the relative importance of envelope and fine structure for lexical-tone perception resembled that for melody recognition rather than that for English speech recognition. Delivering fine-structure information in cochlear implant stimulation could be particularly beneficial for lexical-tone perception.},
	number = {6 Pt 1},
	urldate = {2020-10-30},
	journal = {J Acoust Soc Am},
	author = {Xu, Li and Pfingst, Bryan E.},
	month = dec,
	year = {2003},
	pmid = {14714781},
	pmcid = {PMC1283139},
	pages = {3024--3027},
	file = {PubMed Central Full Text PDF:/home/sivaprakasaman/Zotero/storage/L9WC3FRI/Xu and Pfingst - 2003 - Relative importance of temporal envelope and fine .pdf:application/pdf}
}

@article{arnoldner_speech_2007,
	title = {Speech and music perception with the new fine structure speech coding strategy: preliminary results},
	volume = {127},
	issn = {0001-6489},
	shorttitle = {Speech and music perception with the new fine structure speech coding strategy},
	doi = {10.1080/00016480701275261},
	abstract = {CONCLUSIONS: Taking into account the excellent results with significant improvements in the speech tests and the very high satisfaction of the patients using the new strategy, this first implementation of a fine structure strategy could offer a new quality of hearing with cochlear implants (CIs).
OBJECTIVE: This study consisted of an intra-individual comparison of speech recognition, music perception and patient preference when subjects used two different speech coding strategies with a MedEl Pulsar CI: continuous interleaved sampling (CIS) and the new fine structure processing (FSP) strategy. In contrast to envelope-based strategies, the FSP strategy also delivers subtle pitch and timing differences of sound to the user and is thereby supposed to enhance speech perception in noise and increase the quality of music perception.
PATIENTS AND METHODS: This was a prospective study assessing performance with two different speech coding strategies. The setting was a CI programme at an academic tertiary referral centre. Fourteen post-lingually deaf patients using a MedEl Pulsar CI with a mean CI experience of 0.98 years were supplied with the new FSP speech coding strategy. Subjects consecutively used the two different speech coding strategies. Speech and music tests were performed with the previously fitted CIS strategy, immediately after fitting with the new FSP strategy and 4, 8 and 12 weeks later. The main outcome measures were individual performance and subjective assessment of two different speech processors.
RESULTS: Speech and music test scores improved statistically significantly after conversion from CIS to FSP strategy. Twelve of 14 patients preferred the new FSP speech processing strategy over the CIS strategy.},
	language = {eng},
	number = {12},
	journal = {Acta Otolaryngol},
	author = {Arnoldner, Christoph and Riss, Dominik and Brunner, Markus and Durisin, Martin and Baumgartner, Wolf-Dieter and Hamzavi, Jafar-Sasan},
	month = dec,
	year = {2007},
	pmid = {17851892},
	keywords = {Female, Male, Humans, Adult, Speech Perception, Music, Aged, Middle Aged, Cochlear Implants},
	pages = {1298--1303}
}

@article{riquimaroux_perception_2006,
	title = {Perception of noise-vocoded speech sounds: {Sentences}, words, accents and melodies},
	volume = {27},
	shorttitle = {Perception of noise-vocoded speech sounds},
	doi = {10.1250/ast.27.325},
	abstract = {Recent works on perception of noise-vocoded speech sound (NVSS) have revealed that amplitude envelope information is very important for speech perception when spectral information is not sufficiently available. Basically, the fundamental frequency information is not available and formant peaks cannot not be identified in NVSS. However, we can even recognize accent and distinguish male voice from female voice in NVSS. More, melody can be created from lyrics once lyrics are intelligible. In the present study, findings from fMRI measurement are introduced to show neural activities in the central nervous system during listening to NVSS. The present data indicate that various sites in the brain, which are not ordinarily used for speech recognition, participate in making NVSS intelligible. Applications of the present work include an innovative speech processor and a training system for hearing impaired people.},
	number = {6},
	journal = {Acoustical Science and Technology},
	author = {Riquimaroux, Hiroshi},
	year = {2006},
	keywords = {Speech perception, Amplitude envelope, Brain plasticity, Functional MRI, Noise-vocoded speech sounds},
	pages = {325--331},
	file = {J-Stage - Snapshot:/home/sivaprakasaman/Zotero/storage/9C2TNU2A/en.html:text/html;Full Text PDF:/home/sivaprakasaman/Zotero/storage/TNTENAZT/Riquimaroux - 2006 - Perception of noise-vocoded speech sounds Sentenc.pdf:application/pdf}
}

@article{heng_impaired_2011,
	title = {Impaired perception of temporal fine structure and musical timbre in cochlear implant users},
	volume = {280},
	issn = {0378-5955},
	url = {http://www.sciencedirect.com/science/article/pii/S0378595511001560},
	doi = {10.1016/j.heares.2011.05.017},
	abstract = {Cochlear implant (CI) users demonstrate severe limitations in perceiving musical timbre, a psychoacoustic feature of sound responsible for ‘tone color’ and one’s ability to identify a musical instrument. The reasons for this limitation remain poorly understood. In this study, we sought to examine the relative contributions of temporal envelope and fine structure for timbre judgments, in light of the fact that speech processing strategies employed by CI systems typically employ envelope extraction algorithms. We synthesized “instrumental chimeras” that systematically combined variable amounts of envelope and fine structure in 25\% increments from two different source instruments with either sustained or percussive envelopes. CI users and normal hearing (NH) subjects were presented with 150 chimeras and asked to determine which instrument the chimera more closely resembled in a single-interval two-alternative forced choice task. By combining instruments with similar and dissimilar envelopes, we controlled the valence of envelope for timbre identification and compensated for envelope reconstruction from fine structure information. Our results show that NH subjects utilize envelope and fine structure interchangeably, whereas CI subjects demonstrate overwhelming reliance on temporal envelope. When chimeras were created from dissimilar envelope instrument pairs, NH subjects utilized a combination of envelope (p = 0.008) and fine structure information (p = 0.009) to make timbre judgments. In contrast, CI users utilized envelope information almost exclusively to make timbre judgments (p {\textless} 0.001) and ignored fine structure information (p = 0.908). Interestingly, when the value of envelope as a cue was reduced, both NH subjects and CI users utilized fine structure information to make timbre judgments (p {\textless} 0.001), although the effect was quite weak in CI users. Our findings confirm that impairments in fine structure processing underlie poor perception of musical timbre in CI users.},
	language = {en},
	number = {1},
	urldate = {2020-11-01},
	journal = {Hearing Research},
	author = {Heng, Joseph and Cantarero, Gabriela and Elhilali, Mounya and Limb, Charles J.},
	month = oct,
	year = {2011},
	pages = {192--200},
	file = {ScienceDirect Full Text PDF:/home/sivaprakasaman/Zotero/storage/DUPUCZDZ/Heng et al. - 2011 - Impaired perception of temporal fine structure and.pdf:application/pdf;ScienceDirect Snapshot:/home/sivaprakasaman/Zotero/storage/9PHRDBM9/S0378595511001560.html:text/html}
}

@article{manno_uncertain_2019,
	title = {Uncertain {Emotion} {Discrimination} {Differences} {Between} {Musicians} and {Non}-musicians {Is} {Determined} by {Fine} {Structure} {Association}: {Hilbert} {Transform} {Psychophysics}},
	volume = {13},
	issn = {1662-4548},
	shorttitle = {Uncertain {Emotion} {Discrimination} {Differences} {Between} {Musicians} and {Non}-musicians {Is} {Determined} by {Fine} {Structure} {Association}},
	doi = {10.3389/fnins.2019.00902},
	abstract = {Humans perceive musical sound as a complex phenomenon, which is known to induce an emotional response. The cues used to perceive emotion in music have not been unequivocally elucidated. Here, we sought to identify the attributes of sound that confer an emotion to music and determine if professional musicians have different musical emotion perception than non-musicians. The objective was to determine which sound cues are used to resolve emotional signals. Happy or sad classical music excerpts modified in fine structure or envelope conveying different degrees of emotional certainty were presented. Certainty was determined by identification of the emotional characteristic presented during a forced-choice discrimination task. Participants were categorized as good or poor performers (n = 32, age 21.16 ± 2.59 SD) and in a separate group as musicians in the first or last year of music education at a conservatory (n = 32, age 21.97 ± 2.42). We found that temporal fine structure information is essential for correct emotional identification. Non-musicians used less fine structure information to discriminate emotion in music compared with musicians. The present psychophysical experiments revealed what cues are used to resolve emotional signals and how they differ between non-musicians and musically educated individuals.},
	language = {eng},
	journal = {Front Neurosci},
	author = {Manno, Francis A. M. and Cruces, Raul R. and Lau, Condon and Barrios, Fernando A.},
	year = {2019},
	pmid = {31619943},
	pmcid = {PMC6759500},
	keywords = {envelope, psychophysics, fine structure, amplitude, emotion, frequency, modulation},
	pages = {902},
	file = {Full Text:/home/sivaprakasaman/Zotero/storage/IYT2HD8E/Manno et al. - 2019 - Uncertain Emotion Discrimination Differences Betwe.pdf:application/pdf}
}

@article{parida_spectrally_2020,
	title = {Spectrally specific temporal analyses of spike-train responses to complex sounds: {A} unifying framework},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Spectrally specific temporal analyses of spike-train responses to complex sounds},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.17.208330v1},
	doi = {10.1101/2020.07.17.208330},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Significant scientific and translational questions remain in auditory neuroscience surrounding the neural correlates of perception. Relating perceptual and neural data collected from humans can be useful; however, human-based neural data are typically limited to evoked far-field responses, which lack anatomical and physiological specificity. Laboratory-controlled preclinical animal models offer the advantage of comparing single-unit and evoked responses from the same animals. This ability provides opportunities to develop invaluable insight into proper interpretations of evoked responses, which benefits both basic-science studies of neural mechanisms and translational applications, e.g., diagnostic development. However, these comparisons have been limited by a disconnect between the types of spectrotemporal analyses used with single-unit spike trains and evoked responses, which results because these response types are fundamentally different (point-process versus continuous-valued signals) even though the responses themselves are related. Here, we describe a unifying framework to study temporal coding of complex sounds that allows spike-train and evoked-response data to be analyzed and compared using the same advanced signal-processing techniques. The framework uses alternating-polarity peristimulus-time histograms computed from single-unit spike trains to allow advanced spectral analyses of both slow (envelope) and rapid (temporal fine structure) response components. Demonstrated benefits include: (1) generalization beyond classic metrics of temporal coding, e.g., vector strength and correlograms, (2) novel spectrally specific temporal-coding measures that are less corrupted by distortions due to hair-cell transduction, synaptic rectification, and neural stochasticity compared to previous metrics, e.g., the correlogram peak-height, (3) spectrally specific analyses of spike-train modulation coding that can be directly compared to perceptually based models of speech intelligibility, and (4) superior spectral resolution in analyzing the neural representation of nonstationary sounds, such as speech and music. This unifying framework significantly expands the potential of preclinical animal models to advance our understanding of the physiological correlates of perceptual deficits in real-world listening following sensorineural hearing loss.{\textless}/p{\textgreater}{\textless}h3{\textgreater}Author summary{\textless}/h3{\textgreater} {\textless}p{\textgreater}Despite major technological and computational advances, we remain unable to match human auditory perception using machines, or to restore normal-hearing communication for those with sensorineural hearing loss. An overarching reason for these limitations is that the neural correlates of auditory perception, particularly for complex everyday sounds, remain largely unknown. Although neural responses can be measured in humans noninvasively and compared with perception, these evoked responses lack the anatomical and physiological specificity required to reveal underlying neural mechanisms. Single-unit spike-train responses can be measured from preclinical animal models with well-specified pathology; however, the disparate response types (point-process versus continuous-valued signals) have limited application of the same advanced signal-processing analyses to single-unit and evoked responses required for direct comparison. Here, we fill this gap with a unifying framework for analyzing both spike-train and evoked neural responses using advanced spectral analyses of both the slow and rapid response components that are known to be perceptually relevant for speech and music, particularly in challenging listening environments. Numerous benefits of this framework are demonstrated here, which support its potential to advance the translation of spike-train data from animal models to improve clinical diagnostics and technological development for real-world listening.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-11-01},
	journal = {bioRxiv},
	author = {Parida, Satyabrata and Bharadwaj, Hari and Heinz, Michael G.},
	month = jul,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {2020.07.17.208330},
	file = {Full Text PDF:/home/sivaprakasaman/Zotero/storage/YQ2KS3LY/Parida et al. - 2020 - Spectrally specific temporal analyses of spike-tra.pdf:application/pdf;Snapshot:/home/sivaprakasaman/Zotero/storage/LEQG7T4K/2020.07.17.html:text/html}
}

@article{bruce_phenomenological_2018,
	series = {Computational models of the auditory system},
	title = {A phenomenological model of the synapse between the inner hair cell and auditory nerve: {Implications} of limited neurotransmitter release sites},
	volume = {360},
	issn = {0378-5955},
	shorttitle = {A phenomenological model of the synapse between the inner hair cell and auditory nerve},
	url = {http://www.sciencedirect.com/science/article/pii/S0378595517303696},
	doi = {10.1016/j.heares.2017.12.016},
	abstract = {Peterson and Heil [Hear. Res., In Press] have argued that the statistics of spontaneous spiking in auditory nerve fibers (ANFs) can be best explained by a model with a limited number of synaptic vesicle docking (release) sites (∼4) and a relatively-long average redocking time (∼16–17 ms) for each of the sites. In this paper we demonstrate how their model can be: i) generalized to also describe sound-driven ANF responses and ii) incorporated into a well-established and widely-used model of the entire auditory periphery [Zilany et al., J. Acoust. Soc. Am. 135, 283–286, 2014]. The responses of the new model exhibit substantial improvement in several measures of ANF spiking statistics, and predicted physiological forward-masking and rate-level functions from the new model structure are shown to also better match published physiological data.},
	language = {en},
	urldate = {2020-11-01},
	journal = {Hearing Research},
	author = {Bruce, Ian C. and Erfani, Yousof and Zilany, Muhammad S. A.},
	month = mar,
	year = {2018},
	keywords = {Synapse, Auditory model, Neurotransmitter vesicle release, Refractoriness, Renewal process, Spike timing statistics},
	pages = {40--54},
	file = {ScienceDirect Full Text PDF:/home/sivaprakasaman/Zotero/storage/QH4EDU5C/Bruce et al. - 2018 - A phenomenological model of the synapse between th.pdf:application/pdf;ScienceDirect Snapshot:/home/sivaprakasaman/Zotero/storage/YFK2NZKS/S0378595517303696.html:text/html}
}

@misc{noauthor_sound_nodate,
	title = {Sound samples - https://philharmonia.co.uk/resources/sound-samples/},
	url = {https://philharmonia.co.uk/resources/sound-samples/},
	language = {en-GB},
	urldate = {2020-11-01},
	journal = {Philharmonia},
	file = {Snapshot:/home/sivaprakasaman/Zotero/storage/IU48YD57/sound-samples.html:text/html}
}

@article{li_improved_2012,
	title = {Improved perception of speech in noise and {Mandarin} tones with acoustic simulations of harmonic coding for cochlear implants},
	volume = {132},
	issn = {0001-4966},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3505211/},
	doi = {10.1121/1.4756827},
	abstract = {Harmonic and temporal fine structure (TFS) information are important cues for speech perception in noise and music perception. However, due to the inherently coarse spectral and temporal resolution in electric hearing, the question of how to deliver harmonic and TFS information to cochlear implant (CI) users remains unresolved. A harmonic-single-sideband-encoder [(HSSE); Nie et al. (2008). Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing; Lie et al., (2010). Proceedings of IEEE International Conference on Acoustics, Speech, and Signal Processing] strategy has been proposed that explicitly tracks the harmonics in speech and transforms them into modulators conveying both amplitude modulation and fundamental frequency information. For unvoiced speech, HSSE transforms the TFS into a slowly varying yet still noise-like signal. To investigate its potential, four- and eight-channel vocoder simulations of HSSE and the continuous-interleaved-sampling (CIS) strategy were implemented, respectively. Using these vocoders, five normal-hearing subjects’ speech recognition performance was evaluated under different masking conditions; another five normal-hearing subjects’ Mandarin tone identification performance was also evaluated. Additionally, the neural discharge patterns evoked by HSSE- and CIS-encoded Mandarin tone stimuli were simulated using an auditory nerve model. All subjects scored significantly higher with HSSE than with CIS vocoders. The modeling analysis demonstrated that HSSE can convey temporal pitch cues better than CIS. Overall, the results suggest that HSSE is a promising strategy to enhance speech perception with CIs.},
	number = {5},
	urldate = {2020-11-01},
	journal = {J Acoust Soc Am},
	author = {Li, Xing and Nie, Kaibao and Imennov, Nikita S. and Won, Jong Ho and Drennan, Ward R. and Rubinstein, Jay T. and Atlas, Les E.},
	month = nov,
	year = {2012},
	pmid = {23145619},
	pmcid = {PMC3505211},
	pages = {3387--3398},
	file = {PubMed Central Full Text PDF:/home/sivaprakasaman/Zotero/storage/N7ZZQFPL/Li et al. - 2012 - Improved perception of speech in noise and Mandari.pdf:application/pdf}
}

@article{lee_timbre_2020,
	title = {The {Timbre} {Perception} {Test} ({TPT}): {A} new interactive musical assessment tool to measure timbre perception ability},
	volume = {82},
	issn = {1943-3921},
	shorttitle = {The {Timbre} {Perception} {Test} ({TPT})},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7536169/},
	doi = {10.3758/s13414-020-02058-3},
	abstract = {To date, tests that measure individual differences in the ability to perceive musical timbre are scarce in the published literature. The lack of such tool limits research on how timbre, a primary attribute of sound, is perceived and processed among individuals. The current paper describes the development of the Timbre Perception Test (TPT), in which participants use a slider to reproduce heard auditory stimuli that vary along three important dimensions of timbre: envelope, spectral flux, and spectral centroid. With a sample of 95 participants, the TPT was calibrated and validated against measures of related abilities and examined for its reliability. The results indicate that a short-version (8 minutes) of the TPT has good explanatory support from a factor analysis model, acceptable internal reliability (α = .69, ωt = .70), good test–retest reliability (r = .79) and substantial correlations with self-reported general musical sophistication (ρ = .63) and pitch discrimination (ρ = .56), as well as somewhat lower correlations with duration discrimination (ρ = .27), and musical instrument discrimination abilities (ρ = .33). Overall, the TPT represents a robust tool to measure an individual’s timbre perception ability. Furthermore, the use of sliders to perform a reproductive task has shown to be an effective approach in threshold testing. The current version of the TPT is openly available for research purposes.},
	number = {7},
	urldate = {2020-11-01},
	journal = {Atten Percept Psychophys},
	author = {Lee, Harin and Müllensiefen, Daniel},
	year = {2020},
	pmid = {32529570},
	pmcid = {PMC7536169},
	pages = {3658--3675},
	file = {PubMed Central Full Text PDF:/home/sivaprakasaman/Zotero/storage/YPQKVMPE/Lee and Müllensiefen - 2020 - The Timbre Perception Test (TPT) A new interactiv.pdf:application/pdf}
}

@article{heinz_quantifying_2009,
	title = {Quantifying envelope and fine-structure coding in auditory nerve responses to chimaeric speech},
	volume = {10},
	issn = {1438-7573},
	doi = {10.1007/s10162-009-0169-8},
	abstract = {Any sound can be separated mathematically into a slowly varying envelope and rapidly varying fine-structure component. This property has motivated numerous perceptual studies to understand the relative importance of each component for speech and music perception. Specialized acoustic stimuli, such as auditory chimaeras with the envelope of one sound and fine structure of another have been used to separate the perceptual roles for envelope and fine structure. Cochlear narrowband filtering limits the ability to isolate fine structure from envelope; however, envelope recovery from fine structure has been difficult to evaluate physiologically. To evaluate envelope recovery at the output of the cochlea, neural cross-correlation coefficients were developed that quantify the similarity between two sets of spike-train responses. Shuffled auto- and cross-correlogram analyses were used to compute separate correlations for responses to envelope and fine structure based on both model and recorded spike trains from auditory nerve fibers. Previous correlogram analyses were extended to isolate envelope coding more effectively in auditory nerve fibers with low center frequencies, which are particularly important for speech coding. Recovered speech envelopes were present in both model and recorded responses to one- and 16-band speech fine-structure chimaeras and were significantly greater for the one-band case, consistent with perceptual studies. Model predictions suggest that cochlear recovered envelopes are reduced following sensorineural hearing loss due to broadened tuning associated with outer-hair cell dysfunction. In addition to the within-fiber cross-stimulus cases considered here, these neural cross-correlation coefficients can also be used to evaluate spatiotemporal coding by applying them to cross-fiber within-stimulus conditions. Thus, these neural metrics can be used to quantitatively evaluate a wide range of perceptually significant temporal coding issues relevant to normal and impaired hearing.},
	language = {eng},
	number = {3},
	journal = {J Assoc Res Otolaryngol},
	author = {Heinz, Michael G. and Swaminathan, Jayaganesh},
	month = sep,
	year = {2009},
	pmid = {19365691},
	pmcid = {PMC3084379},
	keywords = {Acoustic Stimulation, Animals, Nerve Fibers, Cochlea, Chinchilla, Speech, Evoked Potentials, Auditory, Cochlear Nerve, Action Potentials, Models, Animal, Computer Simulation, Cochlear Implants},
	pages = {407--423},
	file = {Full Text:/home/sivaprakasaman/Zotero/storage/QSWLFINZ/Heinz and Swaminathan - 2009 - Quantifying envelope and fine-structure coding in .pdf:application/pdf}
}

@article{smith_chimaeric_2002,
	title = {Chimaeric sounds reveal dichotomies in auditory perception},
	volume = {416},
	issn = {0028-0836},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2268248/},
	doi = {10.1038/416087a},
	abstract = {By Fourier's theorem, signals can be decomposed into a sum of sinusoids of different frequencies. This is especially relevant for hearing, because the inner ear performs a form of mechanical Fourier transform by mapping frequencies along the length of the cochlear partition. An alternative signal decomposition, originated by Hilbert, is to factor a signal into the product of a slowly varying envelope and a rapidly varying fine time structure. Neurons in the auditory brainstem– sensitive to these features have been found in mammalian physiological studies. To investigate the relative perceptual importance of envelope and fine structure, we synthesized stimuli that we call ‘auditory chimaeras’, which have the envelope of one sound and the fine structure of another. Here we show that the envelope is most important for speech reception, and the fine structure is most important for pitch perception and sound localization. When the two features are in conflict, the sound of speech is heard at a location determined by the fine structure, but the words are identified according to the envelope. This finding reveals a possible acoustic basis for the hypothesized ‘what’ and ‘where’ pathways in the auditory cortex–.},
	number = {6876},
	urldate = {2020-11-01},
	journal = {Nature},
	author = {Smith, Zachary M. and Delgutte, Bertrand and Oxenham, Andrew J.},
	month = mar,
	year = {2002},
	pmid = {11882898},
	pmcid = {PMC2268248},
	pages = {87--90},
	file = {PubMed Central Full Text PDF:/home/sivaprakasaman/Zotero/storage/4DIPIAQG/Smith et al. - 2002 - Chimaeric sounds reveal dichotomies in auditory pe.pdf:application/pdf}
}

@article{nayagam_spiral_2011,
	title = {The spiral ganglion: connecting the peripheral and central auditory systems},
	volume = {278},
	issn = {0378-5955},
	shorttitle = {The spiral ganglion},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3152679/},
	doi = {10.1016/j.heares.2011.04.003},
	abstract = {In mammals, the initial bridge between the physical world of sound and perception of that sound is established by neurons of the spiral ganglion. The cell bodies of these neurons give rise to peripheral processes that contact acoustic receptors in the organ of Corti, and the central processes collect together to form the auditory nerve that projects into the brain. In order to better understand hearing at this initial stage, we need to know the following about spiral ganglion neurons: (1) their cell biology including cytoplasmic, cytoskeletal, and membrane properties, (2) their peripheral and central connections including synaptic structure; (3) the nature of their neural signaling; and (4) their capacity for plasticity and rehabilitation. In this report, we will update the progress on these topics and indicate important issues still awaiting resolution.},
	number = {1-2},
	urldate = {2020-11-15},
	journal = {Hear Res},
	author = {Nayagam, Bryony A and Muniak, Michael A and Ryugo, David K},
	month = aug,
	year = {2011},
	pmid = {21530629},
	pmcid = {PMC3152679},
	pages = {2--20},
	file = {PubMed Central Full Text PDF:/home/sivaprakasaman/Zotero/storage/4VWTMXSK/Nayagam et al. - 2011 - The spiral ganglion connecting the peripheral and.pdf:application/pdf}
}

@article{cariani_neural_1996,
	title = {Neural correlates of the pitch of complex tones. {II}. {Pitch} shift, pitch ambiguity, phase invariance, pitch circularity, rate pitch, and the dominance region for pitch},
	volume = {76},
	issn = {0022-3077},
	url = {http://journals.physiology.org/doi/abs/10.1152/jn.1996.76.3.1717},
	doi = {10.1152/jn.1996.76.3.1717},
	abstract = {1. The neural correlates of low pitches produced by complex tones were studied by analyzing temporal discharge patterns of auditory nerve fibers in Dial-anesthetized cats. In the previous paper it was observed that, for harmonic stimuli, the most frequent interspike interval present in the population of auditory nerve fibers always corresponded to the perceived pitch (predominant interval hypothesis). The fraction of these most frequent intervals relative to the total number of intervals qualitatively corresponded to strength (salience) of the low pitches that are heard. 2. This paper addresses the neural correlates of stimuli that produce more complex patterns of pitch judgments, such as shifts in pitch and multiple pitches. Correlates of pitch shift and pitch ambiguity were investigated with the use of harmonic and inharmonic amplitude-modulated (AM) tones varying either in carrier frequency or modulation frequency. Pitches estimated from the pooled interval distributions showed shifts corresponding to "the first effect of pitch shift" (de Boer's rule) that is observed psychophysically. Pooled interval distributions in response to inharmonic stimulus segments showed multiple maxima corresponding to the multiple pitches heard by human listeners (pitch ambiguity). 3. AM and quasi-frequency-modulated tones with low carrier frequencies produce very similar patterns of pitch judgments, despite great differences in their phase spectra and waveform envelopes. Pitches estimated from pooled interval distributions were remarkably similar for the two kinds of stimuli, consistent with the psychophysically observed phase invariance of pitches produced by sets of low-frequency components. 4. Trains of clicks having uniform and alternating polarities were used to investigate the relation between pitches associated with periodicity and those associated with click rate. For unipolar click trains, where periodicity and rate coincide, physiologically estimated pitches closely follow the fundamental period. This corresponds to the pitch at the fundamental frequency (F0) that is heard. For alternating click trains, where rate and periodicity do not coincide, physiologically estimated pitches always closely followed the fundamental period. Although these pitch estimates corresponded to periodicity pitches that are heard for F0s {\textgreater} 150 Hz, they did not correspond to the rate pitches that are heard for F0s {\textless} 150 Hz. The predominant interval hypothesis thus failed to predict rate pitch. 5. When alternating-polarity click trains are high-pass filtered, rate pitches are strengthened and can also be heard at F0s {\textgreater} 150 Hz. Pitches for high-pass-filtered alternating click trains were estimated from pooled responses of fibers with characteristic frequencies (CFs) {\textgreater} 2 kHz. Roughly equal numbers of intervals at 1/rate and 1/F0 were found for all F0s studied, from 80 to 160 Hz, producing pitch estimates consistent with the rate pitches that are heard after high-pass filtering. The existence region for rate pitch also coincided with the presence of clear periodicities related to the click rate in pooled peristimulus time histograms. These periodicities were strongest for ensembles of fibers with CFs {\textgreater} 2 kHz, where there is widespread synchrony of discharges across many fibers. 6. The "dominance region for pitch" was studied with the use of two harmonic complexes consisting of harmonics 3-5 of one F0 and harmonics 6-12 of another fundamental 20\% higher in frequency. When the complexes were presented individually, pitch estimates were always close to the fundamental of the complex. When the complexes were presented concurrently, pitch estimates always followed the fundamental of harmonics 3-5 for F0s of 150-480 Hz. For F0s of 125-150 Hz, pitch estimates followed one or the other fundamental, and for F0s {\textless} 125 Hz, pitch estimates followed the fundamental of harmonics 6-12. (ABSTRACT TRUNCATED)},
	number = {3},
	urldate = {2020-11-16},
	journal = {Journal of Neurophysiology},
	author = {Cariani, P. A. and Delgutte, B.},
	month = sep,
	year = {1996},
	note = {Publisher: American Physiological Society},
	pages = {1717--1734},
	file = {Full Text PDF:/home/sivaprakasaman/Zotero/storage/GCZRLXHN/Cariani and Delgutte - 1996 - Neural correlates of the pitch of complex tones. I.pdf:application/pdf;Snapshot:/home/sivaprakasaman/Zotero/storage/HERZK2JH/jn.1996.76.3.html:text/html}
}

@article{heinz_quantifying_2009-1,
	title = {Quantifying envelope and fine-structure coding in auditory nerve responses to chimaeric speech},
	volume = {10},
	issn = {1438-7573},
	doi = {10.1007/s10162-009-0169-8},
	abstract = {Any sound can be separated mathematically into a slowly varying envelope and rapidly varying fine-structure component. This property has motivated numerous perceptual studies to understand the relative importance of each component for speech and music perception. Specialized acoustic stimuli, such as auditory chimaeras with the envelope of one sound and fine structure of another have been used to separate the perceptual roles for envelope and fine structure. Cochlear narrowband filtering limits the ability to isolate fine structure from envelope; however, envelope recovery from fine structure has been difficult to evaluate physiologically. To evaluate envelope recovery at the output of the cochlea, neural cross-correlation coefficients were developed that quantify the similarity between two sets of spike-train responses. Shuffled auto- and cross-correlogram analyses were used to compute separate correlations for responses to envelope and fine structure based on both model and recorded spike trains from auditory nerve fibers. Previous correlogram analyses were extended to isolate envelope coding more effectively in auditory nerve fibers with low center frequencies, which are particularly important for speech coding. Recovered speech envelopes were present in both model and recorded responses to one- and 16-band speech fine-structure chimaeras and were significantly greater for the one-band case, consistent with perceptual studies. Model predictions suggest that cochlear recovered envelopes are reduced following sensorineural hearing loss due to broadened tuning associated with outer-hair cell dysfunction. In addition to the within-fiber cross-stimulus cases considered here, these neural cross-correlation coefficients can also be used to evaluate spatiotemporal coding by applying them to cross-fiber within-stimulus conditions. Thus, these neural metrics can be used to quantitatively evaluate a wide range of perceptually significant temporal coding issues relevant to normal and impaired hearing.},
	language = {eng},
	number = {3},
	journal = {J Assoc Res Otolaryngol},
	author = {Heinz, Michael G. and Swaminathan, Jayaganesh},
	month = sep,
	year = {2009},
	pmid = {19365691},
	pmcid = {PMC3084379},
	keywords = {Acoustic Stimulation, Animals, Nerve Fibers, Cochlea, Chinchilla, Speech, Evoked Potentials, Auditory, Cochlear Nerve, Action Potentials, Models, Animal, Computer Simulation, Cochlear Implants},
	pages = {407--423},
	file = {Full Text:/home/sivaprakasaman/Zotero/storage/VTC7QT9D/Heinz and Swaminathan - 2009 - Quantifying envelope and fine-structure coding in .pdf:application/pdf}
}

@article{klapuri_multiple_2003,
	title = {Multiple fundamental frequency estimation based on harmonicity and spectral smoothness},
	volume = {11},
	issn = {1558-2353},
	doi = {10.1109/TSA.2003.815516},
	abstract = {A new method for estimating the fundamental frequencies of concurrent musical sounds is described. The method is based on an iterative approach, where the fundamental frequency of the most prominent sound is estimated, the sound is subtracted from the mixture, and the process is repeated for the residual signal. For the estimation stage, an algorithm is proposed which utilizes the frequency relationships of simultaneous spectral components, without assuming ideal harmonicity. For the subtraction stage, the spectral smoothness principle is proposed as an efficient new mechanism in estimating the spectral envelopes of detected sounds. With these techniques, multiple fundamental frequency estimation can be performed quite accurately in a single time frame, without the use of long-term temporal features. The experimental data comprised recorded samples of 30 musical instruments from four different sources. Multiple fundamental frequency estimation was performed for random sound source and pitch combinations. Error rates for mixtures ranging from one to six simultaneous sounds were 1.8\%, 3.9\%, 6.3\%, 9.9\%, 14\%, and 18\%, respectively. In musical interval and chord identification tasks, the algorithm outperformed the average of ten trained musicians. The method works robustly in noise, and is able to handle sounds that exhibit inharmonicities. The inharmonicity factor and spectral envelope of each sound is estimated along with the fundamental frequency.},
	number = {6},
	journal = {IEEE Transactions on Speech and Audio Processing},
	author = {Klapuri, A. P.},
	month = nov,
	year = {2003},
	note = {Conference Name: IEEE Transactions on Speech and Audio Processing},
	keywords = {acoustic signal processing, Signal processing, Humans, Speech, Music, hearing, spectral analysis, Frequency estimation, Signal analysis, Signal processing algorithms, acoustic signal analysis, chord identification, concurrent musical sounds, frequency estimation, harmonicity, Instruments, iterative approach, iterative methods, Iterative methods, multiple fundamental frequency estimation, Multiple signal classification, musical acoustics, musical interval, pitch combinations, pitch perceptions, random sound source, spectral components, spectral smoothness},
	pages = {804--816},
	file = {IEEE Xplore Full Text PDF:/home/sivaprakasaman/Zotero/storage/ZGXC8TNE/Klapuri - 2003 - Multiple fundamental frequency estimation based on.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sivaprakasaman/Zotero/storage/P2SF7XSW/1255467.html:text/html}
}

@article{moore_preferred_2016,
	title = {Preferred {Compression} {Speed} for {Speech} and {Music} and {Its} {Relationship} to {Sensitivity} to {Temporal} {Fine} {Structure}},
	volume = {20},
	issn = {2331-2165},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5017572/},
	doi = {10.1177/2331216516640486},
	abstract = {Multichannel amplitude compression is widely used in hearing aids. The preferred compression speed varies across individuals.  suggested that reduced sensitivity to temporal fine structure (TFS) may be associated with preference for slow compression. This idea was tested using a simulated hearing aid. It was also assessed whether preferences for compression speed depend on the type of stimulus: speech or music. Twenty-two hearing-impaired subjects were tested, and the stimulated hearing aid was fitted individually using the CAM2A method. On each trial, a given segment of speech or music was presented twice. One segment was processed with fast compression and the other with slow compression, and the order was balanced across trials. The subject indicated which segment was preferred and by how much. On average, slow compression was preferred over fast compression, more so for music, but there were distinct individual differences, which were highly correlated for speech and music. Sensitivity to TFS was assessed using the difference limen for frequency at 2000 Hz and by two measures of sensitivity to interaural phase at low frequencies. The results for the difference limens for frequency, but not the measures of sensitivity to interaural phase, supported the suggestion that preference for compression speed is affected by sensitivity to TFS.},
	urldate = {2020-11-25},
	journal = {Trends Hear},
	author = {Moore, Brian C. J. and Sęk, Aleksander},
	month = sep,
	year = {2016},
	pmid = {27604778},
	pmcid = {PMC5017572},
	file = {PubMed Central Full Text PDF:/home/sivaprakasaman/Zotero/storage/SHAGFHDY/Moore and Sęk - 2016 - Preferred Compression Speed for Speech and Music a.pdf:application/pdf}
}

@article{kates_comparing_2015,
	title = {Comparing the information conveyed by envelope modulation for speech intelligibility, speech quality, and music quality},
	volume = {138},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4931899},
	doi = {10.1121/1.4931899},
	abstract = {This paper uses mutual information to quantify the relationship between envelope modulation fidelity and perceptual responses. Data from several previous experiments that measured speech intelligibility, speech quality, and music quality are evaluated for normal-hearing and hearing-impaired listeners. A model of the auditory periphery is used to generate envelope signals, and envelope modulation fidelity is calculated using the normalized cross-covariance of the degraded signal envelope with that of a reference signal. Two procedures are used to describe the envelope modulation: (1) modulation within each auditory frequency band and (2) spectro-temporal processing that analyzes the modulation of spectral ripple components fit to successive short-time spectra. The results indicate that low modulation rates provide the highest information for intelligibility, while high modulation rates provide the highest information for speech and music quality. The low-to-mid auditory frequencies are most important for intelligibility, while mid frequencies are most important for speech quality and high frequencies are most important for music quality. Differences between the spectral ripple components used for the spectro-temporal analysis were not significant in five of the six experimental conditions evaluated. The results indicate that different modulation-rate and auditory-frequency weights may be appropriate for indices designed to predict different types of perceptual relationships.},
	number = {4},
	urldate = {2020-11-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Kates, James M. and Arehart, Kathryn H.},
	month = oct,
	year = {2015},
	note = {Publisher: Acoustical Society of America},
	pages = {2470--2482},
	file = {Full Text PDF:/home/sivaprakasaman/Zotero/storage/PAFU49IA/Kates and Arehart - 2015 - Comparing the information conveyed by envelope mod.pdf:application/pdf;Snapshot:/home/sivaprakasaman/Zotero/storage/228CSEGX/1.html:text/html}
}

@article{kong_timbre_2012,
	title = {Timbre and {Speech} {Perception} in {Bimodal} and {Bilateral} {Cochlear}-{Implant} {Listeners}},
	volume = {33},
	issn = {0196-0202},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3428469/},
	doi = {10.1097/AUD.0b013e318252caae},
	number = {5},
	urldate = {2020-11-25},
	journal = {Ear Hear},
	author = {Kong, Ying-Yee and Mullangi, Ala and Marozeau, Jeremy},
	month = sep,
	year = {2012},
	pmid = {22677814},
	pmcid = {PMC3428469},
	pages = {645--659},
	file = {PubMed Central Full Text PDF:/home/sivaprakasaman/Zotero/storage/JA3IG3RB/Kong et al. - 2012 - Timbre and Speech Perception in Bimodal and Bilate.pdf:application/pdf}
}

@article{kong_temporal_2011,
	title = {Temporal and spectral cues for musical timbre perception in electric hearing},
	volume = {54},
	issn = {1558-9102},
	doi = {10.1044/1092-4388(2010/10-0196)},
	abstract = {PURPOSE: The purpose of this study was to investigate musical timbre perception in cochlear-implant (CI) listeners using a multidimensional scaling technique to derive a timbre space. Methods Sixteen stimuli that synthesized western musical instruments were used (McAdams, Winsberg, Donnadieu, De Soete, \& Krimphoff, 1995). Eight CI listeners and 15 normal-hearing (NH) listeners participated. Each listener made judgments of dissimilarity between stimulus pairs. Acoustical analyses that characterized the temporal and spectral characteristics of each stimulus were performed to examine the psychophysical nature of each perceptual dimension.
RESULTS: For NH listeners, the timbre space was best represented in three dimensions, one correlated with the temporal envelope (log-attack time) of the stimuli, one correlated with the spectral envelope (spectral centroid), and one correlated with the spectral fine structure (spectral irregularity) of the stimuli. The timbre space from CI listeners, however, was best represented by two dimensions, one correlated with temporal envelope features and the other weakly correlated with spectral envelope features of the stimuli.
CONCLUSIONS: Temporal envelope was a dominant cue for timbre perception in CI listeners. Compared to NH listeners, CI listeners showed reduced reliance on both spectral envelope and spectral fine structure cues for timbre perception.},
	language = {eng},
	number = {3},
	journal = {J Speech Lang Hear Res},
	author = {Kong, Ying-Yee and Mullangi, Ala and Marozeau, Jeremy and Epstein, Michael},
	month = jun,
	year = {2011},
	pmid = {21060140},
	pmcid = {PMC3107380},
	keywords = {Acoustics, Acoustic Stimulation, Deafness, Female, Male, Humans, Models, Biological, Adult, Young Adult, Auditory Perception, Electric Stimulation, Time Perception, Adolescent, Cues, Music, Middle Aged, Cochlear Implants},
	pages = {981--994},
	file = {Accepted Version:/home/sivaprakasaman/Zotero/storage/UN6WARPX/Kong et al. - 2011 - Temporal and spectral cues for musical timbre perc.pdf:application/pdf}
}

@article{ou_individual_2016,
	title = {Individual differences in processing pitch contour and rise time in adults: {A} behavioral and electrophysiological study of {Cantonese} tone merging},
	volume = {139},
	issn = {1520-8524},
	shorttitle = {Individual differences in processing pitch contour and rise time in adults},
	doi = {10.1121/1.4954252},
	abstract = {One way to understand the relationship between speech perception and production is to examine cases where the two dissociate. This study investigates the hypothesis that perceptual acuity reflected in event-related potentials (ERPs) to rise time of sound amplitude envelope and pitch contour [reflected in the mismatch negativity (MMN)] may associate with individual differences in production among speakers with otherwise comparable perceptual abilities. To test this hypothesis, advantage was taken of an on-going sound change-tone merging in Cantonese, and compared the ERPs between two groups of typically developed native speakers who could discriminate the high rising and low rising tones with equivalent accuracy but differed in the distinctiveness of their production of these tones. Using a passive oddball paradigm, early positive-going EEG components to rise time and MMN to pitch contour were elicited during perception of the two tones. Significant group differences were found in neural responses to rise time rather than pitch contour. More importantly, individual differences in efficiency of tone discrimination in response latency and magnitude of neural responses to rise time were correlated with acoustic measures of F0 offset and rise time differences in productions of the two rising tones.},
	language = {eng},
	number = {6},
	journal = {J Acoust Soc Am},
	author = {Ou, Jinghua and Law, Sam-Po},
	year = {2016},
	pmid = {27369146},
	keywords = {Acoustics, Acoustic Stimulation, Auditory Pathways, Reaction Time, Female, Male, Time Factors, Humans, Audiometry, Speech, Sound Spectrography, Young Adult, Speech Perception, Evoked Potentials, Auditory, Pitch Discrimination, Electroencephalography, Phonetics, Individuality, Speech Acoustics, Speech Production Measurement},
	pages = {3226},
	file = {Submitted Version:/home/sivaprakasaman/Zotero/storage/KE5QL4XB/Ou and Law - 2016 - Individual differences in processing pitch contour.pdf:application/pdf}
}

@article{skuk_influences_2014,
	title = {Influences of fundamental frequency, formant frequencies, aperiodicity, and spectrum level on the perception of voice gender},
	volume = {57},
	issn = {1558-9102},
	doi = {10.1044/1092-4388(2013/12-0314)},
	abstract = {PURPOSE: To determine the relative importance of acoustic parameters (fundamental frequency [F0], formant frequencies [FFs], aperiodicity, and spectrum level [SL]) on voice gender perception, the authors used a novel parameter-morphing approach that, unlike spectral envelope shifting, allows the application of nonuniform scale factors to transform formants and more direct comparison of parameter impact.
METHOD: In each of 2 experiments, 16 listeners with normal hearing (8 female, 8 male) classified voice gender for morphs between female and male speakers, using syllable tokens from 2 male-female speaker pairs. Morphs varied single acoustic parameters (Experiment 1) or selected combinations (Experiment 2), keeping residual parameters androgynous, as determined in a baseline experiment.
RESULTS: The strongest cue related to gender perception was F0, followed by FF and SL. Aperiodicity did not systematically influence gender perception. Morphing F0 and FF in conjunction produced convincing changes in perceived gender-changes that were equivalent to those for Full morphs interpolating all parameters. Despite the importance of F0, morphing FF and SL in combination produced effective changes in voice gender perception.
CONCLUSIONS: The most important single parameters for gender perception are, in order, F0, FF, and SL. At the same time, F0 and vocal tract resonances have a comparable impact on voice gender perception.},
	language = {eng},
	number = {1},
	journal = {J Speech Lang Hear Res},
	author = {Skuk, Verena G. and Schweinberger, Stefan R.},
	month = feb,
	year = {2014},
	pmid = {23882002},
	keywords = {Female, Male, Humans, Models, Biological, Adult, Young Adult, Speech Perception, Cues, Phonetics, Judgment, Voice, Speech Acoustics, Sex Characteristics},
	pages = {285--296}
}

@article{bones_phase_2014,
	title = {Phase locked neural activity in the human brainstem predicts preference for musical consonance},
	volume = {58},
	issn = {1873-3514},
	doi = {10.1016/j.neuropsychologia.2014.03.011},
	abstract = {When musical notes are combined to make a chord, the closeness of fit of the combined spectrum to a single harmonic series (the 'harmonicity' of the chord) predicts the perceived consonance (how pleasant and stable the chord sounds; McDermott, Lehr, \& Oxenham, 2010). The distinction between consonance and dissonance is central to Western musical form. Harmonicity is represented in the temporal firing patterns of populations of brainstem neurons. The current study investigates the role of brainstem temporal coding of harmonicity in the perception of consonance. Individual preference for consonant over dissonant chords was measured using a rating scale for pairs of simultaneous notes. In order to investigate the effects of cochlear interactions, notes were presented in two ways: both notes to both ears or each note to different ears. The electrophysiological frequency following response (FFR), reflecting sustained neural activity in the brainstem synchronised to the stimulus, was also measured. When both notes were presented to both ears the perceptual distinction between consonant and dissonant chords was stronger than when the notes were presented to different ears. In the condition in which both notes were presented to the both ears additional low-frequency components, corresponding to difference tones resulting from nonlinear cochlear processing, were observable in the FFR effectively enhancing the neural harmonicity of consonant chords but not dissonant chords. Suppressing the cochlear envelope component of the FFR also suppressed the additional frequency components. This suggests that, in the case of consonant chords, difference tones generated by interactions between notes in the cochlea enhance the perception of consonance. Furthermore, individuals with a greater distinction between consonant and dissonant chords in the FFR to individual harmonics had a stronger preference for consonant over dissonant chords. Overall, the results provide compelling evidence for the role of neural temporal coding in the perception of consonance, and suggest that the representation of harmonicity in phase locked neural firing drives the perception of consonance.},
	language = {eng},
	journal = {Neuropsychologia},
	author = {Bones, Oliver and Hopkins, Kathryn and Krishnan, Ananthanarayan and Plack, Christopher J.},
	month = may,
	year = {2014},
	pmid = {24690415},
	pmcid = {PMC4040538},
	keywords = {Acoustic Stimulation, Neurons, Frequency following response, Female, Male, Humans, Adult, Young Adult, Auditory Perception, Brain Stem, Evoked Potentials, Adolescent, Pitch, Auditory brainstem, Music, Harmonicity, Individual differences, Musical consonance},
	pages = {23--32},
	file = {Full Text:/home/sivaprakasaman/Zotero/storage/57XSDA9P/Bones et al. - 2014 - Phase locked neural activity in the human brainste.pdf:application/pdf}
}

@article{kalathottukaren_prosody_2015,
	title = {Prosody perception and musical pitch discrimination in adults using cochlear implants},
	volume = {54},
	issn = {1708-8186},
	doi = {10.3109/14992027.2014.997314},
	abstract = {OBJECTIVES: This study investigated prosodic perception and musical pitch discrimination in adults using cochlear implants (CI), and examined the relationship between prosody perception scores and non-linguistic auditory measures, demographic variables, and speech recognition scores.
DESIGN: Participants were given four subtests of the PEPS-C (profiling elements of prosody in speech-communication), the adult paralanguage subtest of the DANVA 2 (diagnostic analysis of non verbal accuracy 2), and the contour and interval subtests of the MBEA (Montreal battery of evaluation of amusia).
STUDY SAMPLE: Twelve CI users aged 25;5 to 78;0 years participated.
RESULTS: CI participants performed significantly more poorly than normative values for New Zealand adults for PEPS-C turn-end, affect, and contrastive stress reception subtests, but were not different from the norm for the chunking reception subtest. Performance on the DANVA 2 adult paralanguage subtest was lower than the normative mean reported by Saindon (2010) . Most of the CI participants performed at chance level on both MBEA subtests.
CONCLUSION: CI users have difficulty perceiving prosodic information accurately. Difficulty in understanding different aspects of prosody and music may be associated with reduced pitch perception ability.},
	language = {eng},
	number = {7},
	journal = {Int J Audiol},
	author = {Kalathottukaren, Rose Thomas and Purdy, Suzanne C. and Ballard, Elaine},
	month = jul,
	year = {2015},
	pmid = {25634773},
	keywords = {Cochlear implants, Female, Male, Humans, Adult, Speech Perception, Pitch Discrimination, Music, Aged, Middle Aged, Speech Discrimination Tests, Cochlear Implants, Linguistics, musical interval, diagnostic analysis of non verbal accuracy 2, Montreal battery of evaluation of amusia, musical contour, New Zealand, Nonverbal Communication, profiling elements of prosody in speech-communication},
	pages = {444--452}
}

@article{croghan_music_2014,
	title = {Music preferences with hearing aids: effects of signal properties, compression settings, and listener characteristics},
	volume = {35},
	issn = {1538-4667},
	shorttitle = {Music preferences with hearing aids},
	doi = {10.1097/AUD.0000000000000056},
	abstract = {OBJECTIVES: Current knowledge of how to design and fit hearing aids to optimize music listening is limited. Many hearing-aid users listen to recorded music, which often undergoes compression limiting (CL) in the music industry. Therefore, hearing-aid users may experience twofold effects of compression when listening to recorded music: music-industry CL and hearing-aid wide dynamic-range compression (WDRC). The goal of this study was to examine the roles of input-signal properties, hearing-aid processing, and individual variability in the perception of recorded music, with a focus on the effects of dynamic-range compression.
DESIGN: A group of 18 experienced hearing-aid users made paired-comparison preference judgments for classical and rock music samples using simulated hearing aids. Music samples were either unprocessed before hearing-aid input or had different levels of music-industry CL. Hearing-aid conditions included linear gain and individually fitted WDRC. Combinations of four WDRC parameters were included: fast release time (50 msec), slow release time (1,000 msec), three channels, and 18 channels. Listeners also completed several psychophysical tasks.
RESULTS: Acoustic analyses showed that CL and WDRC reduced temporal envelope contrasts, changed amplitude distributions across the acoustic spectrum, and smoothed the peaks of the modulation spectrum. Listener judgments revealed that fast WDRC was least preferred for both genres of music. For classical music, linear processing and slow WDRC were equally preferred, and the main effect of number of channels was not significant. For rock music, linear processing was preferred over slow WDRC, and three channels were preferred to 18 channels. Heavy CL was least preferred for classical music, but the amount of CL did not change the patterns of WDRC preferences for either genre. Auditory filter bandwidth as estimated from psychophysical tuning curves was associated with variability in listeners' preferences for classical music.
CONCLUSIONS: Fast, multichannel WDRC often leads to poor music quality, whereas linear processing or slow WDRC are generally preferred. Furthermore, the effect of WDRC is more important for music preferences than music-industry CL applied to signals before the hearing-aid input stage. Variability in hearing-aid users' perceptions of music quality may be partially explained by frequency resolution abilities.},
	language = {eng},
	number = {5},
	journal = {Ear Hear},
	author = {Croghan, Naomi B. H. and Arehart, Kathryn H. and Kates, James M.},
	month = oct,
	year = {2014},
	pmid = {25010635},
	keywords = {Acoustic Stimulation, Female, Male, Humans, Hearing Aids, Hearing Loss, Sensorineural, Music, Aged, Middle Aged, Aged, 80 and over, Consumer Behavior},
	pages = {e170--184}
}

@article{croghan_music_2014-1,
	title = {Music {Preferences} {With} {Hearing} {Aids}: {Effects} of {Signal} {Properties}, {Compression} {Settings}, and {Listener} {Characteristics}},
	volume = {35},
	issn = {0196-0202},
	shorttitle = {Music {Preferences} {With} {Hearing} {Aids}},
	url = {https://journals.lww.com/ear-hearing/Abstract/2014/09000/Music_Preferences_With_Hearing_Aids___Effects_of.13.aspx},
	doi = {10.1097/AUD.0000000000000056},
	abstract = {Objectives: 
        Current knowledge of how to design and fit hearing aids to optimize music listening is limited. Many hearing-aid users listen to recorded music, which often undergoes compression limiting (CL) in the music industry. Therefore, hearing-aid users may experience twofold effects of compression when listening to recorded music: music-industry CL and hearing-aid wide dynamic-range compression (WDRC). The goal of this study was to examine the roles of input-signal properties, hearing-aid processing, and individual variability in the perception of recorded music, with a focus on the effects of dynamic-range compression.
        Design: 
        A group of 18 experienced hearing-aid users made paired-comparison preference judgments for classical and rock music samples using simulated hearing aids. Music samples were either unprocessed before hearing-aid input or had different levels of music-industry CL. Hearing-aid conditions included linear gain and individually fitted WDRC. Combinations of four WDRC parameters were included: fast release time (50 msec), slow release time (1,000 msec), three channels, and 18 channels. Listeners also completed several psychophysical tasks.
        Results: 
        Acoustic analyses showed that CL and WDRC reduced temporal envelope contrasts, changed amplitude distributions across the acoustic spectrum, and smoothed the peaks of the modulation spectrum. Listener judgments revealed that fast WDRC was least preferred for both genres of music. For classical music, linear processing and slow WDRC were equally preferred, and the main effect of number of channels was not significant. For rock music, linear processing was preferred over slow WDRC, and three channels were preferred to 18 channels. Heavy CL was least preferred for classical music, but the amount of CL did not change the patterns of WDRC preferences for either genre. Auditory filter bandwidth as estimated from psychophysical tuning curves was associated with variability in listeners’ preferences for classical music.
        Conclusions: 
        Fast, multichannel WDRC often leads to poor music quality, whereas linear processing or slow WDRC are generally preferred. Furthermore, the effect of WDRC is more important for music preferences than music-industry CL applied to signals before the hearing-aid input stage. Variability in hearing-aid users’ perceptions of music quality may be partially explained by frequency resolution abilities.},
	language = {en-US},
	number = {5},
	urldate = {2020-11-25},
	journal = {Ear and Hearing},
	author = {Croghan, Naomi B. H. and Arehart, Kathryn H. and Kates, James M.},
	month = oct,
	year = {2014},
	pages = {e170},
	file = {Snapshot:/home/sivaprakasaman/Zotero/storage/TBCTQETW/Music_Preferences_With_Hearing_Aids___Effects_of.13.html:text/html}
}

@article{donofrio_musical_2020,
	title = {Musical {Emotion} {Perception} in {Bimodal} {Patients}: {Relative} {Weighting} of {Musical} {Mode} and {Tempo} {Cues}},
	volume = {14},
	issn = {1662-453X},
	shorttitle = {Musical {Emotion} {Perception} in {Bimodal} {Patients}},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2020.00114/full},
	doi = {10.3389/fnins.2020.00114},
	abstract = {Several cues are used to convey musical emotion, the two primary being musical mode and musical tempo. Specifically, major and minor modes tend to be associated with positive and negative valence, respectively, and songs at fast tempi have been associated with more positive valence compared to songs at slow tempi (Balkwill \& Thompson, 1999; Webster \& Weir, 2005). In Experiment I, we examined the relative weighting of musical tempo and musical mode among adult cochlear implant (CI) users combining electric and contralateral acoustic stimulation, or “bimodal” hearing. Our primary hypothesis was that bimodal listeners would utilize both tempo and mode cues in their musical emotion judgments in a manner similar to normal-hearing listeners. Our secondary hypothesis was that low-frequency spectral resolution in the non-implanted ear, as quantified via psychophysical tuning curves at 262 and 440 Hz, would be significantly correlated with degree of bimodal benefit for musical emotion perception. In Experiment II, we investigated across-channel spectral resolution using a spectral modulation detection (SMD) task and neural representation of temporal fine structure via the frequency following response (FFR) for a 170-ms /da/ stimulus. Results indicate that CI-alone performance was driven almost exclusively by tempo cues, whereas bimodal listening demonstrated use of both tempo and mode. Additionally, bimodal benefit for musical emotion perception may be correlated with spectral resolution in the non-implanted ear via SMD, as well as neural representation of F0 envelope via FFR – though further study with a larger sample size is warranted. Thus, contralateral acoustic hearing can offer significant benefit for musical emotion perception, and the degree of benefit may be dependent upon spectral resolution of the non-implanted ear.},
	language = {English},
	urldate = {2020-11-25},
	journal = {Front. Neurosci.},
	author = {D’Onofrio, Kristen L. and Caldwell, Meredith and Limb, Charles and Smith, Spencer and Kessler, David M. and Gifford, René H.},
	year = {2020},
	note = {Publisher: Frontiers},
	keywords = {Hearing Loss, Cochlear Implants, music perception, musical emotion, spectral modulation detection, Bimodal, Frequency Following Response (FFR), hearing aid (HA), Psychophysical tuning curves (PTCs)},
	file = {Full Text PDF:/home/sivaprakasaman/Zotero/storage/AYL2KGCB/D’Onofrio et al. - 2020 - Musical Emotion Perception in Bimodal Patients Re.pdf:application/pdf}
}

@article{kumar_brain_2014,
	title = {A brain basis for musical hallucinations},
	volume = {52},
	issn = {1973-8102},
	doi = {10.1016/j.cortex.2013.12.002},
	abstract = {The physiological basis for musical hallucinations (MH) is not understood. One obstacle to understanding has been the lack of a method to manipulate the intensity of hallucination during the course of experiment. Residual inhibition, transient suppression of a phantom percept after the offset of a masking stimulus, has been used in the study of tinnitus. We report here a human subject whose MH were residually inhibited by short periods of music. Magnetoencephalography (MEG) allowed us to examine variation in the underlying oscillatory brain activity in different states. Source-space analysis capable of single-subject inference defined left-lateralised power increases, associated with stronger hallucinations, in the gamma band in left anterior superior temporal gyrus, and in the beta band in motor cortex and posteromedial cortex. The data indicate that these areas form a crucial network in the generation of MH, and are consistent with a model in which MH are generated by persistent reciprocal communication in a predictive coding hierarchy.},
	language = {eng},
	journal = {Cortex},
	author = {Kumar, Sukhbinder and Sedley, William and Barnes, Gareth R. and Teki, Sundeep and Friston, Karl J. and Griffiths, Timothy D.},
	month = mar,
	year = {2014},
	pmid = {24445167},
	pmcid = {PMC3969291},
	keywords = {Auditory cortex, Female, Humans, Auditory Perception, Auditory Cortex, Music, Aged, Magnetoencephalography, Predictive coding, Brain, Beta oscillations, Gamma oscillations, Hallucinations, Musical hallucinations},
	pages = {86--97},
	file = {Full Text:/home/sivaprakasaman/Zotero/storage/SWJLGSE6/Kumar et al. - 2014 - A brain basis for musical hallucinations.pdf:application/pdf}
}

@inproceedings{arifianto_enhanced_2016,
	title = {Enhanced harmonics for music appreciation on cochlear implant},
	doi = {10.1109/TENCON.2016.7848410},
	abstract = {The temporal fine structure in music consists of harmonics which is important for music appreciation. The lack of this temporal fine structure in the most used cochlear implant encoding strategy, contributes their lack of the music appreciation. The purpose of this paper is to enhance the harmonics for music appreciation on cochlear implant user. In the first experiment we processed the prior music signal with voice coder (vocoder). During prior processing, the harmonics of original music was degraded. In the second experiment, we used well known signal enhancement, Frequency-Amplitude-Modulation-Encoding (FAME), to enhance the harmonics of prior music. Objective test was conducted to measure the quality of music relative to original music using log spectral distance (LSD). The music quality between the prior and after enhancement are 2,3 and 2,1, respectively. The higher value of LSD means that the enhanced music got worsens. We also conducted subjective listening test from two subjects, 10 musicians and 10 non-musicians, whose their music appreciation is decreasing after the music was enhanced by FAME. It reveals that FAME strategy is not useful to enhance harmonics in signal processing of cochlear implant.},
	booktitle = {2016 {IEEE} {Region} 10 {Conference} ({TENCON})},
	author = {Arifianto, D. and Pratiwi, E. W.},
	month = nov,
	year = {2016},
	note = {ISSN: 2159-3450},
	keywords = {Bandwidth, Frequency modulation, Cochlear implants, temporal fine structure, Music, cochlear implants, vocoder, amplitude, frequency, Multiple signal classification, cochlear implant signal processing, frequency-amplitude-modulation-encoding, Harmonic analysis, log spectral distance, music, music appreciation, music quality, music signal, signal enhancement, vocoders, Vocoders, voice coder},
	pages = {2167--2171},
	file = {IEEE Xplore Abstract Record:/home/sivaprakasaman/Zotero/storage/KYJKL47J/7848410.html:text/html}
}

@inproceedings{kumagai_classification_2017,
	title = {Classification of familiarity based on cross-correlation features between {EEG} and music},
	doi = {10.1109/EMBC.2017.8037458},
	abstract = {An approach to recognize the familiarity of a listener with music using both the electroencephalogram (EEG) signals and the music signal is proposed in this paper. Eight participants listened to melodies produced by piano sounds as simple natural stimuli. We classified the familiarity of each participant using cross-correlation values between EEG and the envelope of the music signal as features of the support vector machine (SVM) or neural network used. Here, we report that the maximum classification accuracy was 100\% obtained by the SVM. These results suggest that the familiarity of music can be classified by cross-correlation values. The proposed approach can be used to recognize high-level brain states such as familiarity, preference, and emotion.},
	booktitle = {2017 39th {Annual} {International} {Conference} of the {IEEE} {Engineering} in {Medicine} and {Biology} {Society} ({EMBC})},
	author = {Kumagai, Y. and Arvaneh, M. and Okawa, H. and Wada, T. and Tanaka, T.},
	month = jul,
	year = {2017},
	note = {ISSN: 1558-4615},
	keywords = {Recognition (Psychology), Electrodes, Electroencephalography, Music, EEG signal, electroencephalography, neural network, Multiple signal classification, music, music signal, Biological neural networks, brain, cross-correlation features, electroencephalogram signals, feature extraction, high-level brain states, Indexes, maximum classification accuracy, medical signal processing, neural nets, Neural Networks (Computer), piano sounds, signal classification, simple natural stimuli, support vector machine, Support Vector Machine, support vector machines, Support vector machines, SVM},
	pages = {2879--2882},
	file = {IEEE Xplore Abstract Record:/home/sivaprakasaman/Zotero/storage/ES6DCGKG/8037458.html:text/html;Accepted Version:/home/sivaprakasaman/Zotero/storage/V6SDYJEK/Kumagai et al. - 2017 - Classification of familiarity based on cross-corre.pdf:application/pdf}
}

@article{zuk_revisiting_2017,
	title = {Revisiting the "enigma" of musicians with dyslexia: {Auditory} sequencing and speech abilities},
	volume = {146},
	issn = {1939-2222},
	shorttitle = {Revisiting the "enigma" of musicians with dyslexia},
	doi = {10.1037/xge0000281},
	abstract = {Previous research has suggested a link between musical training and auditory processing skills. Musicians have shown enhanced perception of auditory features critical to both music and speech, suggesting that this link extends beyond basic auditory processing. It remains unclear to what extent musicians who also have dyslexia show these specialized abilities, considering often-observed persistent deficits that coincide with reading impairments. The present study evaluated auditory sequencing and speech discrimination in 52 adults comprised of musicians with dyslexia, nonmusicians with dyslexia, and typical musicians. An auditory sequencing task measuring perceptual acuity for tone sequences of increasing length was administered. Furthermore, subjects were asked to discriminate synthesized syllable continua varying in acoustic components of speech necessary for intraphonemic discrimination, which included spectral (formant frequency) and temporal (voice onset time [VOT] and amplitude envelope) features. Results indicate that musicians with dyslexia did not significantly differ from typical musicians and performed better than nonmusicians with dyslexia for auditory sequencing as well as discrimination of spectral and VOT cues within syllable continua. However, typical musicians demonstrated superior performance relative to both groups with dyslexia for discrimination of syllables varying in amplitude information. These findings suggest a distinct profile of speech processing abilities in musicians with dyslexia, with specific weaknesses in discerning amplitude cues within speech. Because these difficulties seem to remain persistent in adults with dyslexia despite musical training, this study only partly supports the potential for musical training to enhance the auditory processing skills known to be crucial for literacy in individuals with dyslexia. (PsycINFO Database Record},
	language = {eng},
	number = {4},
	journal = {J Exp Psychol Gen},
	author = {Zuk, Jennifer and Bishop-Liebler, Paula and Ozernov-Palchik, Ola and Moore, Emma and Overy, Katie and Welch, Graham and Gaab, Nadine},
	month = apr,
	year = {2017},
	pmid = {28383990},
	pmcid = {PMC6481192},
	keywords = {Female, Male, Humans, Speech, Adult, Sound Spectrography, Young Adult, Auditory Perception, Speech Perception, Time Perception, Pitch Discrimination, Adolescent, Cues, Music, Phonetics, Dyslexia, Serial Learning},
	pages = {495--511},
	file = {Full Text:/home/sivaprakasaman/Zotero/storage/JZT3ZDY4/Zuk et al. - 2017 - Revisiting the enigma of musicians with dyslexia.pdf:application/pdf}
}

@article{kumagai_familiarity_2017,
	title = {Familiarity {Affects} {Entrainment} of {EEG} in {Music} {Listening}},
	volume = {11},
	issn = {1662-5161},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5526927/},
	doi = {10.3389/fnhum.2017.00384},
	abstract = {Music perception involves complex brain functions. The relationship between music and brain such as cortical entrainment to periodic tune, periodic beat, and music have been well investigated. It has also been reported that the cerebral cortex responded more strongly to the periodic rhythm of unfamiliar music than to that of familiar music. However, previous works mainly used simple and artificial auditory stimuli like pure tone or beep. It is still unclear how the brain response is influenced by the familiarity of music. To address this issue, we analyzed electroencelphalogram (EEG) to investigate the relationship between cortical response and familiarity of music using melodies produced by piano sounds as simple natural stimuli. The cross-correlation function averaged across trials, channels, and participants showed two pronounced peaks at time lags around 70 and 140 ms. At the two peaks the magnitude of the cross-correlation values were significantly larger when listening to unfamiliar and scrambled music compared to those when listening to familiar music. Our findings suggest that the response to unfamiliar music is stronger than that to familiar music. One potential application of our findings would be the discrimination of listeners' familiarity with music, which provides an important tool for assessment of brain activity.},
	urldate = {2020-11-25},
	journal = {Front Hum Neurosci},
	author = {Kumagai, Yuiko and Arvaneh, Mahnaz and Tanaka, Toshihisa},
	month = jul,
	year = {2017},
	pmid = {28798673},
	pmcid = {PMC5526927},
	file = {PubMed Central Full Text PDF:/home/sivaprakasaman/Zotero/storage/5D66PLDM/Kumagai et al. - 2017 - Familiarity Affects Entrainment of EEG in Music Li.pdf:application/pdf}
}

@misc{noauthor_dominant_nodate,
	title = {Dominant {Melody} {Enhancement} in {Cochlear} {Implants} - {IEEE} {Conference} {Publication}},
	url = {https://ieeexplore-ieee-org.ezproxy.lib.purdue.edu/document/8659661},
	urldate = {2020-11-25},
	file = {Dominant Melody Enhancement in Cochlear Implants - IEEE Conference Publication:/home/sivaprakasaman/Zotero/storage/G2H3JG5K/8659661.html:text/html}
}

@article{cheng_music_2018,
	title = {Music {Training} {Can} {Improve} {Music} and {Speech} {Perception} in {Pediatric} {Mandarin}-{Speaking} {Cochlear} {Implant} {Users}},
	volume = {22},
	issn = {2331-2165},
	doi = {10.1177/2331216518759214},
	abstract = {Due to limited spectral resolution, cochlear implants (CIs) do not convey pitch information very well. Pitch cues are important for perception of music and tonal language; it is possible that music training may improve performance in both listening tasks. In this study, we investigated music training outcomes in terms of perception of music, lexical tones, and sentences in 22 young (4.8 to 9.3 years old), prelingually deaf Mandarin-speaking CI users. Music perception was measured using a melodic contour identification (MCI) task. Speech perception was measured for lexical tones and sentences presented in quiet. Subjects received 8 weeks of MCI training using pitch ranges not used for testing. Music and speech perception were measured at 2, 4, and 8 weeks after training was begun; follow-up measures were made 4 weeks after training was stopped. Mean baseline performance was 33.2\%, 76.9\%, and 45.8\% correct for MCI, lexical tone recognition, and sentence recognition, respectively. After 8 weeks of MCI training, mean performance significantly improved by 22.9, 14.4, and 14.5 percentage points for MCI, lexical tone recognition, and sentence recognition, respectively ( p {\textless} .05 in all cases). Four weeks after training was stopped, there was no significant change in posttraining music and speech performance. The results suggest that music training can significantly improve pediatric Mandarin-speaking CI users' music and speech perception.},
	language = {eng},
	journal = {Trends Hear},
	author = {Cheng, Xiaoting and Liu, Yangwenyi and Shu, Yilai and Tao, Duo-Duo and Wang, Bing and Yuan, Yasheng and Galvin, John J. and Fu, Qian-Jie and Chen, Bing},
	month = dec,
	year = {2018},
	pmid = {29484971},
	pmcid = {PMC5833165},
	keywords = {Deafness, Female, Male, Humans, Speech Perception, Pitch Perception, Music, Child, cochlear implant, Cochlear Implants, Child, Preschool, China, pitch, melodic contour identification, music training},
	pages = {2331216518759214},
	file = {Full Text:/home/sivaprakasaman/Zotero/storage/HDW4BKGD/Cheng et al. - 2018 - Music Training Can Improve Music and Speech Percep.pdf:application/pdf}
}

@article{ogg_acoustic_2019,
	title = {Acoustic {Correlates} of {Auditory} {Object} and {Event} {Perception}: {Speakers}, {Musical} {Timbres}, and {Environmental} {Sounds}},
	volume = {10},
	issn = {1664-1078},
	shorttitle = {Acoustic {Correlates} of {Auditory} {Object} and {Event} {Perception}},
	doi = {10.3389/fpsyg.2019.01594},
	abstract = {Human listeners must identify and orient themselves to auditory objects and events in their environment. What acoustic features support a listener's ability to differentiate the great variety of natural sounds they might encounter? Studies of auditory object perception typically examine identification (and confusion) responses or dissimilarity ratings between pairs of objects and events. However, the majority of this prior work has been conducted within single categories of sound. This separation has precluded a broader understanding of the general acoustic attributes that govern auditory object and event perception within and across different behaviorally relevant sound classes. The present experiments take a broader approach by examining multiple categories of sound relative to one another. This approach bridges critical gaps in the literature and allows us to identify (and assess the relative importance of) features that are useful for distinguishing sounds within, between and across behaviorally relevant sound categories. To do this, we conducted behavioral sound identification (Experiment 1) and dissimilarity rating (Experiment 2) studies using a broad set of stimuli that leveraged the acoustic variability within and between different sound categories via a diverse set of 36 sound tokens (12 utterances from different speakers, 12 instrument timbres, and 12 everyday objects from a typical human environment). Multidimensional scaling solutions as well as analyses of item-pair-level responses as a function of different acoustic qualities were used to understand what acoustic features informed participants' responses. In addition to the spectral and temporal envelope qualities noted in previous work, listeners' dissimilarity ratings were associated with spectrotemporal variability and aperiodicity. Subsets of these features (along with fundamental frequency variability) were also useful for making specific within or between sound category judgments. Dissimilarity ratings largely paralleled sound identification performance, however the results of these tasks did not completely mirror one another. In addition, musical training was related to improved sound identification performance.},
	language = {eng},
	journal = {Front Psychol},
	author = {Ogg, Mattson and Slevc, L. Robert},
	year = {2019},
	pmid = {31379658},
	pmcid = {PMC6650748},
	keywords = {timbre, auditory object, acoustics, speaker identification, environmental sound},
	pages = {1594},
	file = {Full Text:/home/sivaprakasaman/Zotero/storage/Y787BKF9/Ogg and Slevc - 2019 - Acoustic Correlates of Auditory Object and Event P.pdf:application/pdf}
}

@misc{noauthor_rayyan_nodate,
	title = {Rayyan {QCRI}},
	url = {https://rayyan.qcri.org/reviews/179605},
	urldate = {2020-11-25},
	file = {Rayyan QCRI:/home/sivaprakasaman/Zotero/storage/MU48VBDD/179605.html:text/html}
}

@article{bidelman_psychophysical_2014,
	title = {Psychophysical auditory filter estimates reveal sharper cochlear tuning in musicians},
	volume = {136},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.4885484},
	doi = {10.1121/1.4885484},
	abstract = {Musicianship confers enhancements to hearing at nearly all levels of the auditory system from periphery to percept. Musicians' superior psychophysical abilities are particularly evident in spectral discrimination and noise-degraded listening tasks, achieving higher perceptual sensitivity than their nonmusician peers. Greater spectral acuity implies that musicianship may increase auditory filter selectivity. This hypothesis was directly tested by measuring both forward- and simultaneous-masked psychophysical tuning curves. Sharper filter tuning (i.e., higher Q10) was observed in musicians compared to nonmusicians. Findings suggest musicians' pervasive listening benefits may be facilitated, in part, by superior spectral processing/decomposition as early as the auditory periphery.},
	number = {1},
	urldate = {2020-11-29},
	journal = {The Journal of the Acoustical Society of America},
	author = {Bidelman, Gavin M. and Schug, Jonathan M. and Jennings, Skyler G. and Bhagat, Shaum P.},
	month = jun,
	year = {2014},
	note = {Publisher: Acoustical Society of America},
	pages = {EL33--EL39},
	file = {Full Text PDF:/home/sivaprakasaman/Zotero/storage/PUDM3ZZR/Bidelman et al. - 2014 - Psychophysical auditory filter estimates reveal sh.pdf:application/pdf;Snapshot:/home/sivaprakasaman/Zotero/storage/7UCCGHP5/1.html:text/html}
}

@article{koelsch_toward_2011,
	title = {Toward a {Neural} {Basis} of {Music} {Perception} – {A} {Review} and {Updated} {Model}},
	volume = {2},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2011.00110/full},
	doi = {10.3389/fpsyg.2011.00110},
	abstract = {Music perception involves acoustic analysis, auditory memory, auditory scene analysis, processing of interval relations, of musical syntax and semantics, and activation of (pre)motor representations of actions. Moreover, music percep- tion potentially elicits emotions, thus giving rise to the modulation of emotional effector systems such as the subjective feeling system, the autonomic nervous system, the hormonal, and the immune system. Building on a previous article (Koelsch \&amp; Siebel, 2005), this review presents an updated model of music percep- tion and its neural correlates. The article describes processes involved in music perception, and reports EEG and fMRI studies that inform about the time course of these processes, as well as about where in the brain these processes might be located.},
	language = {English},
	urldate = {2020-12-01},
	journal = {Front. Psychol.},
	author = {Koelsch, Stefan},
	year = {2011},
	note = {Publisher: Frontiers},
	keywords = {fMRI, EEG, Music, Brain, ERAN, semantics},
	file = {Full Text PDF:/home/sivaprakasaman/Zotero/storage/9KDAXV92/Koelsch - 2011 - Toward a Neural Basis of Music Perception – A Revi.pdf:application/pdf}
}

@article{katsiamis_practical_2007,
	title = {Practical {Gammatone}-{Like} {Filters} for {Auditory} {Processing}},
	volume = {2007},
	issn = {1687-4722},
	url = {https://doi.org/10.1155/2007/63685},
	doi = {10.1155/2007/63685},
	abstract = {This paper deals with continuous-time filter transfer functions that resemble tuning curves at particular set of places on the basilar membrane of the biological cochlea and that are suitable for practical VLSI implementations. The resulting filters can be used in a filterbank architecture to realize cochlea implants or auditory processors of increased biorealism. To put the reader into context, the paper starts with a short review on the gammatone filter and then exposes two of its variants, namely, the differentiated all-pole gammatone filter (DAPGF) and one-zero gammatone filter (OZGF), filter responses that provide a robust foundation for modeling cochlea transfer functions. The DAPGF and OZGF responses are attractive because they exhibit certain characteristics suitable for modeling a variety of auditory data: level-dependent gain, linear tail for frequencies well below the center frequency, asymmetry, and so forth. In addition, their form suggests their implementation by means of cascades of N identical two-pole systems which render them as excellent candidates for efficient analog or digital VLSI realizations. We provide results that shed light on their characteristics and attributes and which can also serve as "design curves" for fitting these responses to frequency-domain physiological data. The DAPGF and OZGF responses are essentially a "missing link" between physiological, electrical, and mechanical models for auditory filtering.},
	language = {en},
	number = {1},
	urldate = {2020-12-03},
	journal = {J AUDIO SPEECH MUSIC PROC.},
	author = {Katsiamis, AG and Drakakis, EM and Lyon, RF},
	month = dec,
	year = {2007},
	pages = {063685},
	file = {Springer Full Text PDF:/home/sivaprakasaman/Zotero/storage/KCKE55CX/Katsiamis et al. - 2007 - Practical Gammatone-Like Filters for Auditory Proc.pdf:application/pdf}
}

@article{bidelman_enhanced_2011,
	title = {Enhanced brainstem encoding predicts musicians’ perceptual advantages with pitch},
	volume = {33},
	copyright = {© 2010 The Authors. European Journal of Neuroscience © 2010 Federation of European Neuroscience Societies and Blackwell Publishing Ltd},
	issn = {1460-9568},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/j.1460-9568.2010.07527.x},
	doi = {https://doi.org/10.1111/j.1460-9568.2010.07527.x},
	abstract = {Important to Western tonal music is the relationship between pitches both within and between musical chords; melody and harmony are generated by combining pitches selected from the fixed hierarchical scales of music. It is of critical importance that musicians have the ability to detect and discriminate minute deviations in pitch in order to remain in tune with other members of their ensemble. Event-related potentials indicate that cortical mechanisms responsible for detecting mistuning and violations in pitch are more sensitive and accurate in musicians as compared with non-musicians. The aim of the present study was to address whether this superiority is also present at a subcortical stage of pitch processing. Brainstem frequency-following responses were recorded from musicians and non-musicians in response to tuned (i.e. major and minor) and detuned (± 4\% difference in frequency) chordal arpeggios differing only in the pitch of their third. Results showed that musicians had faster neural synchronization and stronger brainstem encoding for defining characteristics of musical sequences regardless of whether they were in or out of tune. In contrast, non-musicians had relatively strong representation for major/minor chords but showed diminished responses for detuned chords. The close correspondence between the magnitude of brainstem responses and performance on two behavioral pitch discrimination tasks supports the idea that musicians’ enhanced detection of chordal mistuning may be rooted at pre-attentive, sensory stages of processing. Findings suggest that perceptually salient aspects of musical pitch are not only represented at subcortical levels but that these representations are also enhanced by musical experience.},
	language = {en},
	number = {3},
	urldate = {2020-12-10},
	journal = {European Journal of Neuroscience},
	author = {Bidelman, Gavin M. and Krishnan, Ananthanarayan and Gandour, Jackson T.},
	year = {2011},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1460-9568.2010.07527.x},
	keywords = {auditory evoked potentials, experience-dependent plasticity, fundamental frequency-following response, human, music, pitch discrimination},
	pages = {530--538},
	file = {Full Text PDF:/home/sivaprakasaman/Zotero/storage/6GAH5BEF/Bidelman et al. - 2011 - Enhanced brainstem encoding predicts musicians’ pe.pdf:application/pdf;Snapshot:/home/sivaprakasaman/Zotero/storage/TMNTUPFD/j.1460-9568.2010.07527.html:text/html}
}
